Risk ID	System Component	Risk	Risk Description	Mitigation Controls IDs	Mitigation Controls IDs  New Line	Mitigation Controls	DASF  Revision	Predictive ML Models	RAG - LLMs	Fine-tuned LLMs	Pre-trained LLMs	Foundational LLMs	External Models	Initial AI Risk Impacts	Business Impacts	AI Novelty	MITRE ATLAS as of Q3 2024	MITRE ATTACK as of Q3 2024	OWASP LLM Top 10 2025	OWASP ML Top 10 v0.3	NIST - 800- 53 - Rev 5	NIST 800-53 Controls Mapping Rationale	HITRUST	ENISA’s Securing ML Algorithms	ISO 42001:2023 Controls Objectives and Controls (Annex A)	ISO 27001:2022 Information Security Control Reference (Annex A)	EU AI Act
Raw Data 1.1	Data operations	Raw Data 1.1: Insufficient access controls	Effective access management is fundamental to data security, ensuring only authorized individuals or groups can access specific datasets. Such security protocols encompass authentication, authorization and finely tuned access controls tailored to the scope of access required by each user, down to the file or record level. Establishing definitive governance policies for data access is imperative in response to the heightened risks from data breaches and regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). These policies guard against unauthorized use and are a cornerstone of preserving data integrity and maintaining customer trust.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 51,DASF 55,DASF 59	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 51 DASF 55 DASF 59	• DASF 1: SSO with IdP and MFA to authenticate and limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to authorize access to data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 51: Share data and AI assets securely • DASF 55: Monitor audit logs to detect and respond to threats. • DASF 59: Use clean rooms to collaborate in a secure environment	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Unauthorised access to customer data • Unauthorised access to training data  • Data Breach • Regulatory non-compliance • Loss of Data Integrity	• Breach of legal obligations • Breach of regulatory obligations • Customer Retention • Legal fines • Response costs • Regulatory Fines and Judgement	CyberSec	AML.TA0004: Initial Access AML.T0020: Poison Training Data	ID: T1486 Data Encrypted for Impact ID: T1530 Data from Cloud Storage	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM06.3: Sensitive Information Disclosure LLM08.5, 7: Excessive Agency	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege,  AC-17:Remote Access,  AC-18:Wireless Access,  AC-19:Access Control for Mobile Devices,  IA-2:Identification and Authentication (Organizational Users), IA-5:Authenticator Management,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting	These NIST controls focus on access management, authorization, and least privilege principles, aligning with the access control.	no mapping	Apply a Role Based Access Control (RBAC) model respecting the least privileged principle. Ensure ML applications comply with identity management, authentication, and access control policies.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.15 Logging A.8.16 Monitoring Activities A.5.14 Information transfer A.5.34 Privacy and protection of personal identifiable information (PII)	no mapping
Raw Data 1.2	Data operations	Raw Data 1.2: Missing data classification 	Data classification is critical for data governance, enabling organizations to effectively sort and categorize data by sensitivity, importance and criticality. As data volumes grow exponentially, prioritizing sensitive information protection, risk reduction and data quality becomes imperative. Classification facilitates the implementation of appropriate security measures and governance policies by evaluating data’s risk and value. A robust classification strategy strengthens data governance, mitigates risks, and ensures data integrity and security on a scalable level.	DASF 6	DASF 6	• DASF 6: Classify data with tags as it is ingested into the platform aligning with the organization’s governance requirements	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Data Breach • Breach of Legal Obligations • Breach of Regulatory Obligations • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0043 Craft Adversarial Data AML.T0020: Poison Training Data AML.T0022: Lack of Auditing/Logging	no mapping	LLM03.2, 4, 5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure	ML02:2023 Data Poisoning Attack	AC-1:Policy and Procedures to AC-23:Data Mining Protection, AT-1:Policy and Procedures to AT-6:Training Feedback, AU-1:Policy and Procedures to AU-10:Non-repudiation, MP-1:Policy and Procedures to MP-8:Media Downgrading	NIST controls related to media marking and protection for ensuring data is properly classified and handled according to its sensitivity.	no mapping	Ensure ML applications comply with data security requirements. Control all data used by the ML model. 	A.7.4 Quality of data for AI systems	A.5.12 Classification of information	no mapping
Raw Data 1.3	Data operations	Raw Data 1.3: Poor data quality	Data quality is crucial for reliable data-driven decisions and is a cornerstone of data governance. Malicious actors threaten data integrity, accuracy and consistency, challenging the analytics and decision-making processes that depend on high-quality data, just as a well-intentioned user with poor-quality data can limit the efficacy of an AI system. To safeguard against these threats, organizations must rigorously evaluate key data attributes — accuracy, completeness, freshness and rule compliance. Prioritizing data quality enables organizations to trace data lineage, apply data quality rules and monitor changes, ensuring analytical accuracy and cost-effectiveness.	DASF 7,DASF 21,DASF 36	DASF 7 DASF 21 DASF 36	• DASF 7: Enforce data quality checks on batch and streaming datasets  • DASF 21: Monitor data and AI system from a single pane of glass  • DASF 36: Set up monitoring alerts	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Using Untrusted Data to Make Data-Driven Decisions • Undesired Business Outcomes • Regulatory non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Customer Retention • Regulatory Fines and Judgement	CyberSec	AML.T0020: Poison Training Data AML.T0019: Published Poisoned Datasets AML.T0029: Denial of ML Service	no mapping	LLM03.2, 4, 5: Training Data Poisoning LLM04: Model Denial of Service LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM08: Excessive Agency LLM10: Model Theft	ML08:2023 Model Skewing	SI-2:Flaw Remediation,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity,  AU-3:Content of Audit Records,  AC-6:Least Privilege,  CM-3:Configuration Change Control,  CM-4:Impact Analyses	Audit records and integrity verification for ensuring data quality and traceability across operations.	no mapping	Ensure ML applications comply with data security requirements. Control all data used by the ML model.  Use methods to clean the training dataset from suspicious samples. 	A.7.4 Quality of data for AI systems A.7.6 Data preparation A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs 	no mapping	Article 10.3: Data and Data Governance
Raw Data 1.4	Data operations	Raw Data 1.4: Ineffective storage and encryption	Insecure data storage leaves organizations vulnerable to unauthorized access, potentially leading to data breaches with significant legal, financial and reputational consequences. Encrypting data at rest can help to render the data unreadable to unauthorized actors who bypass security measures or attempt large-scale data exfiltration. Additionally, compliance with industry-specific data security regulations often necessitates such measures.	DASF 5,DASF 8,DASF 9	DASF 5 DASF 8 DASF 9	• DASF 5: Control access to data and other objects for metadata encryption across all data assets • DASF 8: Encrypt data at rest • DASF 9: Encrypt data in transit	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Unauthorised Access to Customer Data • Unauthorised Access to Training Data  • Data Breach • Model Theft • Breach of Legal Obligations • Breach of Regulatory Obligations • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement • Loss of Competitive Advantage • Proprietary Data Loss	CyberSec	AML.T0051 LLM Prompt Injection AML.T0020: Poison Training Data AML.T0002.000: Acquire Public ML Artifacts: Models	ID: T1486 Data Encrypted for Impact ID: T1530 Data from Cloud Storage	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SC-13:Cryptographic Protection,  SC-28:Protection of Information at Rest 	These controls cover encryption of data at rest and in transit, addressing insecure storage and encryption risks.	no mapping	Ensure ML applications comply with data security requirements.	no mapping	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.24 Use of cryptography	no mapping
Raw Data 1.5	Data operations	Raw Data 1.5: Lack of data versioning 	When data gets corrupted by a malicious user by introducing a new set of data or by corrupting a data pipeline, you will need to be able to roll back or trace back to the original data.	DASF 10	DASF 10	• DASF 10: Version data and track change logs on large-scale datasets that are fed to your models	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Inability to Recover from an Incident • Disruption of Business Operations and Service Availability • Regulatory Non-compliance	• Breach of Service Level Agreements • Customer Retention • Regulatory Fines and Judgement • Direct Business Interruption • Fines from SLA	CyberSec	AML.T0020: Poison Training Data AML.T0019: Published Poisoned Datasets	ID: T1565 Data Manipulation	LLM03.2, 4, 5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure	no mapping	CM-2:Baseline Configuration, CM-3:Configuration Change Control,  CP:Contingency Planning Family	This control relates to ensuring software and data maintain integrity, including versioning control.	no mapping	Ensure ML applications comply with data security requirements. Control all data used by the ML model.	A.4.3 Data resources A.7.2 Data for development and enhancement of AI system	no mapping	no mapping
Raw Data 1.6	Data operations	Raw Data 1.6: Insufficient data lineage	Because data may come from multiple sources and go through multiple transformations over its lifecycle, understanding data transparency and usage requirements in AI training is important to risk management. Many compliance regulations require organizations to have a clear understanding and traceability of data used for AI. Data lineage helps organizations be compliant and audit-ready, thereby alleviating the operational overhead of manually creating the trails of data flows for audit reporting purposes.	DASF 11,DASF 51	DASF 11 DASF 51	• DASF 11: Capture and view data lineage • DASF 51: Share data and AI assets securely	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Loss of Data Integrity • Limited Ability to Meet Regulatory Compliance • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0020: Poison Training Data AML.T0019: Published Poisoned Datasets AML.T0010: ML Supply Chain Compromise	no mapping	LLM03.2, 4, 5: Training Data Poisoning LLM05.1, 5: Supply Chain Vulnerabilities,  LLM06.3: Sensitive Information Disclosure	no mapping	AU-3:Content of Audit Records,  AU-6:Audit Record Review, Analysis, and Reporting,  AU-12:Audit Record Generation,  SI-7:Software, Firmware, and Information Integrity,  CM-3:Configuration Change Control,  CM-8:System Component Inventory,  PT-2:Authority to Process Personally Identifiable Information,  RA-3:Risk Assessment	These controls ensure audit mechanisms are in place to track and log actions, which is critical for data lineage.	no mapping	Ensure ML applications comply with data security requirements. Control all data used by the ML model.	A.4.3 Data resources A.7.3 Acquisition of data	A.5.14 Information transfer	no mapping
Raw Data 1.7	Data operations	Raw Data 1.7: Lack of data trustworthiness	Attackers may tamper with or poison raw input data (training data, RAG data, etc). Adversaries may exploit public datasets, which often resemble those used by targeted organizations. To mitigate these threats, organizations should validate data sources, implement integrity checks, and utilize AI and machine learning for anomaly detection.	DASF 10,DASF 51,DASF 59	DASF 10 DASF 51 DASF 59	• DASF 10: Version data and track change logs on large-scale datasets that are fed to your models • DASF 51: Share data and AI assets securely  • DASF 59: Use clean rooms to collaborate in a secure environment	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Compromised Integrity of AI System Outputs • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0020: Poison Training Data AML.T0019: Published Poisoned Datasets AML.T0010: ML Supply Chain Compromise	no mapping	LLM03.2, 4, 5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure	no mapping	SR-2: Supply Chain Risk Management Plan SR-5: Acquisition Strategies, Tools, and Methods SR-6: Supplier Assessments and Reviews SI-4: System Monitoring SI-7(8): Auditing Capability for Significant Events AU-6: Audit Record Review, Analysis, and Reporting AU-9: Protection of Audit Information RA-5: Vulnerability Monitoring and Scanning SC-28: Protection of Information at Rest CP-9: System Backup	Data integrity mechanisms ensure data trustworthiness, preventing poisoning and manipulation.	Supply chain attacks -> Compromised 3rd-party training datasets	Control all data used by the ML model. Ensure reliable sources are used. Use methods to clean the training dataset from suspicious samples. Ensure that models respect differential privacy to a sufficient degree.	A.4.3 Data resources A.7.2 Data for development and enhancement of AI system	A.5.14 Information transfer A.5.34 Privacy and protection of personal identifiable information (PII)	no mapping
Raw Data 1.8	Data operations	Raw Data 1.8: Legality of data	Intellectual property concerns of training data and and legal mandates — such as those from GDPR, CCPA and LGPD — necessitate the capability of machine learning systems to “delete” specific data. But you often can’t “untrain” a model; during the training process, input data is encoded into the internal representation of the model, characterized by elements like thresholds and weights, which could become subject to legal constraints. Tracking your training data and retraining your model using clean and ownership-verified datasets is essential for meeting regulatory demands.	DASF 12,DASF 27,DASF 29	DASF 12 DASF 27 DASF 29	• DASF 12: Delete records from datasets and retrain models to forget data subjects • DASF 27: Pretrain a large language model (LLM) to only use the data that is allowed with LLMs for inference • DASF 29: Build MLOps workflows to track models and trace data sources and lineage to retrain models with the updated dataset by following legal constraints	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Using Data to Train Models without Legal Ownership of Data • Using Data to Train Models without Consent from the Data Owner  • Constrained use of your model • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0057: LLM Data Leakage	no mapping	LLM05.5, 7: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM10.4: Model Theft  	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML06:2023 AI Supply Chain Attacks ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	PT-1: Policy and Procedures  PT-2: Authority to Process Personally Identifiable Information PT-3: Personally Identifiable Information Processing Purposes  PT-4: Consent  PT-5: Privacy Notice  AC-14: Permitted Actions without Identification or Authentication  AC-20: Use of External Systems  SI-12: Information Management and Retention  IR-8: Incident Response Plan  PL-5: Privacy Impact Assessment	Controls for ensuring compliance with legal mandates like data deletion requirements and protection from unauthorized disclosure.	Threats inherent to language models -> Copyright-infringing output	Assess the regulations and laws the ML application must comply with. Ensure ML applications comply with data security requirements. Ensure ML applications comply with third parties' security requirements. Ensure ML applications comply with security policies. Ensure that models respect differential privacy to a sufficient degree.	A.2.3 Alignment with other organizational policies A.3.2 AI roles and responsibilities A.7.2 Data for development and enhancement of AI system A.9.3 Objectives for responsible use of AI system A.4.3 Data resources A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible design and development of AI systems A.6.2.3 Documentation of AI system design and development A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring	A.5.34 Privacy and protection of personal identifiable information (PII) A.8.10 Information deletion	no mapping
Raw Data 1.9	Data operations	Raw Data 1.9: Stale data	When downstream data is not timely or accurate, business processes can be delayed, significantly affecting overall efficiency. Attackers may deliberately target these systems with attacks like denial of service, which can undermine the model’s performance and dependability. It’s crucial to proactively counteract these threats. Data streaming and performance monitoring help protect against such risks, maintaining the input data integrity and ensuring they are delivered promptly to the model.	DASF 7,DASF 13	DASF 7 DASF 13	• DASF 7: Enforce data quality checks on batch and streaming datasets • DASF 13: Use near real-time data for fault-tolerant, near real-time data ingestion, processing and machine learning, and AI for streaming data	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Unreliable Downstream Systems Performance  • Unreliable Downstream Systems Reliability • Regulatory non-compliance • Loss of AI System Integrity	• Customer Retention • Regulatory Fines and Judgement	CyberSec	AML.T0020: Poison Training Data AML.T0021: Backdoor Attacks AML.T0010: ML Supply Chain Compromise	no mapping	LLM03.2, 4, 5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure	ML08:2023 Model Skewing	SI-4: System Monitoring  SI-7: Software, Firmware, and Information Integrity  CP-6: Alternate Storage Site  CP-7: Alternate Processing Site  CP-9: System Backup  AU-10: Non-repudiation  SC-5: Denial-of-Service Protection	Focus on monitoring data for accuracy and timeliness, critical to managing stale data issues.	no mapping	Ensure ML applications comply with data security requirements. Control All Data Used by the ML Model. Define and Monitor Indicators for Proper Functioning of the Model. Ensure that the Model is Sufficiently Resilient to the Environment in which it Will Operate. Implement Processes to Maintain Security Levels of ML Components over Time.	A.7.4 Quality of data for AI systems A.7.6 Data preparation A.7.2 Data for development and enhancement of AI system	no mapping	no mapping
Raw Data 1.10	Data operations	Raw Data 1.10: Lack of data access logs	Without proper audit mechanisms, an organization may not be fully aware of its risk surface area, leaving it vulnerable to data breaches and regulatory noncompliance. Therefore, a well-designed audit team within a data governance or security governance organization is critical in ensuring data security and compliance with regulations such as GDPR and CCPA. By implementing effective data access auditing strategies, organizations can maintain the trust of their customers and protect their data from unauthorized access or misuse.	DASF 14,DASF 55	DASF 14 DASF 55	• DASF 14: Audit actions performed on datasets  • DASF 55: Monitor audit logs 	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Inability to Detect AI System Compromise • Inability to Detect a Cyber Attack on AI Systems • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0020: Poison Training Data AML.T0010: ML Supply Chain Compromise	no mapping	LLM05.5, 7: Supply Chain Vulnerabilitie	ML02:2023 Data Poisoning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack	AU-2: Audit Event Collection AU-3: Content of Audit Records AU-4: Audit Log Storage Capacity AU-5: Response to Audit Processing Failures AU-6: Audit Record Review, Analysis, and Reporting AU-7: Audit Reduction and Report Generation AU-9: Protection of Audit Information AU-11: Audit Record Retention	These controls establish audit mechanisms and monitoring, critical for logging access to data.	no mapping	Include ML Applications in Asset Management Processes. Ensure ML Applications Comply with Data Security Requirements. Ensure ML Applications Comply with Identity Management, Authentication, and Access Control Policies.	A.6.2.8 AI system recording of event logs A.6.2.6 AI system operation and monitoring	A.8.15 Logging A.8.16 Monitoring Activities	Article 12.3: Record-Keeping
Raw Data 1.11	Data operations	Raw Data 1.11: Compromised 3rd-party datasets	Adversaries may poison training data and publish it to public locations or manipulate source datasets known to be used in the AI system. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via supply chain compromise.	DASF 5,DASF 7,DASF 11,DASF 17	DASF 5 DASF 7 DASF 11 DASF 17	• DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 7: Enforce data quality checks on batch and streaming datasets for data sanity checks, and automatically detect anomalies before they make it to the datasets • DASF 11: Capture and view data lineage to capture the lineage all the way to the original raw data sources • DASF 17: Track and reproduce the training data used for ML model training and identify ML models and runs derived from a particular dataset	DASF v 2.0	Yes	Yes	Yes	Yes	No	No	• Unintended Training • Compromised Integrity of AI System Outputs • Unreliable Downstream Systems Reliability • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	AML.T0019  Publish Poisoned Datasets AML.T0010: ML Supply Chain Compromise	ID: T1565 Data Manipulation	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities	ML02:2023 Data Poisoning Attack ML06:2023 AI Supply Chain Attacks ML08:2023 Model Skewing	SI-4: System Monitoring, SI-7: Software, Firmware, and Information Integrity, SR-2: Supply Chain Risk Management Plan, SR-4: Provenance, SR-6: Supplier Assessments and Reviews, SA-8: Security and Privacy Engineering Principles, SA-11: Developer Testing and Evaluation		Compromised 3rd-party training datasets	Ensure Reliable Sources Are Used. Control All Data Used by the ML Model. Conduct a Risk Analysis of the ML Application. Ensure ML Applications Comply with Third Parties' Security Requirements. Check the Vulnerabilities of the Components Used So That They Have an Appropriate Security Level.	A.7.4 Quality of data for AI systems  A.7.6 Data preparation A.4.3 Data resources A.7.3 Acquisition of data A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.5 Data provenance	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code	no mapping
Data Prep 2.1	Data operations	Data Prep 2.1: Preprocessing integrity	Preprocessing includes numerical transformations, data aggregation, text or image data encoding, and new feature creation, followed by combining data by joining tables or merging datasets. Data preparation involves cleaning and formatting tasks such as handling missing values, ensuring correct formats and removing unnecessary columns. Insiders or external actors can introduce errors or manipulate data during preprocessing or from the information repository itself.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 7,DASF 11,DASF 15,DASF 16,DASF 42,DASF 52,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 7 DASF 11 DASF 15 DASF 16 DASF 42 DASF 52 DASF 55	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 7: Enforce data quality checks on batch and streaming datasets for data sanity checks and automatically detect anomalies before they make it to the datasets • DASF 11: Capture and view data lineage to capture the lineage all the way to the original raw data sources • DASF 15: Explore datasets and identify problems • DASF 16: Secure model features to reduce the risk of malicious actors manipulating the features that feed into ML training • DASF 42: Data-centric MLOps and LLMOps promote models as code • DASF 52: Source Code Control  • DASF 55: Monitor audit logs	DASF v 1.0	Yes	No	No	No	No	No	• Errors in Data During Preprocessing • Manipulation of Data During Preprocessing • Compromised Integrity of AI System Outputs • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	no mapping	ID: T1565 Data Manipulation	no mapping	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-7:System and Information Integrity family AC:Access Control family SC- 28:Protection of Information at Rest,  SA-3:System Development Life Cycle ,  SA-4:Acquisition Process,  PE-2:Physical Access Authorizations,  PE-3:Physical Access Control,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  SR-4:Provenance,  SR-6:Supplier Assessments and Reviews	Ensures the integrity of data preprocessing through integrity controls and secure handling of features and models.	no mapping	no mapping	A.4.3 Data resources A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.6 Data preparation A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.5 Data provenance A.4.4 Tooling resources 	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.25 Secure development life cycle A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Data Prep 2.2	Data operations	Data Prep 2.2: Feature manipulation	In almost all cases, raw data requires preprocessing and transformation before it is used to build a model. This process, known as feature engineering, involves converting raw data into structured features, the building blocks of the model. Feature engineering is critical to quality and effectiveness of the model. However, how data are annotated into features can introduce the risk of incorporating attacker biases into an AI/ML system. This can compromise the integrity and accuracy of the model and is a significant security concern for models used in critical decision-making (e.g., financial forecasting, fraud detection).	DASF 1,DASF 2,DASF 3,DASF 4,DASF 16,DASF 42	DASF 1 DASF 2 DASF 3 DASF 4 DASF 16 DASF 42	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 16: Secure model features to prevent and track unauthorized updates to features and for lineage or traceability • DASF 42: Data-centric MLOps and LLMOps promote models as code	DASF v 1.0	Yes	No	No	No	No	No	• Compromised Fairness in AI System Outputs • Bias in AI System Outputs • Untrusted Data used by Decision-making Models • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	AML.TA0011: Impact, disrupt availability or compromise integrity by manipulating business and operational processes	ID: T1565 Data Manipulation	no mapping	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement,  AC-6:Least Privilege,  CM-3:Configuration Change Control,  CM-4:Impact Analyses,  AU-9:Protection of Audit Information,  SI-7:Software, Firmware, and Information Integrity	Controls ensuring data and feature integrity during preprocessing and feature engineering.	Poisoning attacks -> Data poisoning	no mapping	A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.8.3 Information access restriction A.8.2 Privileged access rights	no mapping
Data Prep 2.3	Data operations	Data Prep 2.3: Raw data criteria	An attacker who understands raw data selection criteria may be able to introduce malicious input that compromises system integrity or functionality later in the model lifecycle. Exploitation of this knowledge allows the attacker to bypass established security measures and manipulate the system’s output or behavior. Implementing stringent security measures to safeguard against such manipulations is essential for maintaining the integrity and reliability of ML systems.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 43,DASF 42	DASF 1 DASF 2 DASF 3 DASF 4 DASF 43 DASF 42	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to restrict the IP addresses that can authenticate to Databricks  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 43: Use access control lists to control access to data, data streams and notebooks • DASF 42: Data-centric MLOps and LLMOps unit and integration testing	DASF v 1.0	Yes	No	No	No	No	No	• Bypass of AI System Security Measures • Manipulation of Model Behaviour • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	AML.TA0011: Impact, disrupt availability or compromise integrity by manipulating business and operational processes	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM10: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management,  AC-3:Access Enforcement,  AC-6:Least Privilege,  SC-7:Boundary Protection,  SC-8:Transmission Confidentiality and Integrity,  SC-28:Protection of Information at Rest,  MP:Media Protection Family,  SI-7:Software, Firmware, and Information Integrity,  SI-9:Information Input Restrictions,  SI-10:Information Input Validation	Ensures input data validation and filtering to mitigate risks of data manipulation during preprocessing.	Poisoning attacks -> Data poisoning	Control All Data Used by the ML Model. Use methods to clean the training dataset from suspicious samples. Add some adversarial examples to the training dataset. Enlarge the training dataset. Define and monitor indicators for proper functioning of the model. Implement tools to detect if a data point is an adversarial example or not. Apply a RBAC model, respecting the least privilege principle. Ensure ML projects follow the global process for integrating security into projects. Integrate ML specificities to awareness strategy and ensure all ML stakeholders are receiving it. Implement processes to maintain security levels of ML components over time.	A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.8.3 Information access restriction A.8.2 Privileged access rights A.5.15 Access control	no mapping
Data Prep 2.4	Data operations	Data Prep 2.4: Adversarial partitions	If an attacker can influence the partitioning of datasets used in training and evaluation, they can effectively exercise indirect control over the ML system by making them vulnerable to adversarial attacks, where carefully crafted inputs lead to incorrect outputs. These attacks can exploit the space partitioning capabilities of machine learning models, such as tree ensembles and neural networks, leading to misclassifications even in high-confidence scenarios. This form of “model control” can lead to biased or compromised outcomes. Therefore, it is crucial that datasets accurately reflect the intended operational reality of the ML system. Implementing stringent security measures to safeguard against such manipulations is essential for maintaining the integrity and reliability of ML systems.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 17,DASF 42	DASF 1 DASF 2 DASF 3 DASF 4 DASF 17 DASF 42	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to restrict the IP addresses that can authenticate to Databricks  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 17: Track and reproduce the training data used for ML model training to track and reproduce the training data partitions and the human owner accountable for ML model training, as well as identify ML models and runs derived from a particular dataset • DASF 42: Data-centric MLOps and LLMOps for unit and integration testing	DASF v 1.0	Yes	No	No	No	No	No	• Unauthorised Influence of Training Data Partitions • Attacker Model Control • Biased or Compromised AI System Outputs • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	AML.TA0011: Impact, disrupt availability or compromise integrity by manipulating business and operational processes	ID: T1565 Data Manipulation	LLM03: Training Data Poisoning	ML02:2023 Data Poisoning Attack,  ML03:2023 Model Inversion Attack,  ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management,  AC-3:Access Enforcement,  AC-6:Least Privilege,  SI-6:Security and Privacy Function Verification,  SI-7:Software, Firmware, and Information Integrity,  SI-19:De-identification,  CM-3:Configuration Change Control,  CM-6:Configuration Settings,  SC-28:Protection of Information at Rest	Ensures partitioning security to prevent adversaries from manipulating ML model partitions.	no mapping	Control All Data Used by the ML Model. Use methods to clean the training dataset from suspicious samples. Add some adversarial examples to the training dataset. Define and monitor indicators for proper functioning of the model. Implement tools to detect if a data point is an adversarial example or not . Apply a RBAC model, respecting the least privilege principle. Ensure ML projects follow the global process for integrating security into projects. Integrate ML specificities to awareness strategy and ensure all ML stakeholders are receiving it.	A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.4.3 Data resources A.7 Data for AI systems A.7.4 Quality of data for AI systems 	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks 	no mapping
Datasets 3.1	Data operations	Datasets 3.1: Data poisoning	Attackers can compromise an ML system by contaminating its training data to manipulate its output at the inference stage. All three initial components of a typical ML system — raw data, data preparation and datasets — are susceptible to poisoning attacks. Intentionally manipulated data, possibly coordinated across these components, derail the ML training process and create an unreliable model. Practitioners must assess the potential extent of training data an attacker might control internally and externally and the resultant risks.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 7,DASF 11,DASF 14,DASF 16,DASF 17,DASF 51,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 7 DASF 11 DASF 14 DASF 16 DASF 17 DASF 51 DASF 55	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to restrict the IP addresses that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 7: Enforce data quality checks on batch and streaming datasets for data sanity checks, and automatically detect anomalies before they make it to the datasets • DASF 11: Capture and view data lineage to capture the lineage all the way to the original raw data sources • DASF 14: Audit actions performed on datasets • DASF 16: Secure model features • DASF 17: Track and reproduce the training data used for ML model training and identify ML models and runs derived from a particular dataset • DASF 51: Share data and AI assets securely • DASF 55: Monitor audit logs	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Manipulation of Model Outputs or AI System Outputs • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement	AI Sec	AML.T0043: Craft Adversarial Data AML.T0002.000: Aquire Datasets AML.T0019: Publish Poisoned Datasets	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack  ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity,  SR-4:Provenance,  SR-6:Supplier Assessments and Reviews,  CM-3:Configuration Change Control,  AC-3:Access Enforcement,  AC-6:Least Privilege,  PL-8:Security and Privacy Architectures,  SC-28:Protection of Information at Rest	Data integrity controls, ensuring protection against poisoning attacks through least privilege and integrity checks.	Poisoning attacks -> Data poisoning	Control all data used by the ML model. Use methods to clean the training dataset from suspicious samples. Ensure reliable sources are used. Enlarge the training dataset. Integrate poisoning control after the "model evaluation" phase. Define and monitor indicators for proper functioning of the model. Integrate ML specificities to awareness strategy and ensure all ML stakeholders are receiving it.	A.4.3 Data resources A.7.3 Acquisition of data A.6.2.8 AI system recording of event logs A.6.2.6 AI system operation and monitoring	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.8.3 Information access restriction A.8.2 Privileged access rights A.5.15 Access control A.8.15 Logging A.5.14 Information transfer A.8.16 Monitoring Activities	no mapping
Datasets 3.2	Data operations	Datasets 3.2: Ineffective storage and encryption	Data stored and managed insecurely pose significant risks, especially for ML systems. It’s crucial to consider who has access to training datasets and the reasons behind this access. While access controls are a vital mitigation strategy, their effectiveness is limited with public data sources, where traditional security measures may not apply. Therefore, it’s essential to ask: What are the implications if an attacker gains access and control over your data sources? Understanding and preparing for this scenario is critical for safeguarding the integrity of ML systems.	DASF 5,DASF 8,DASF 9	DASF 5 DASF 8 DASF 9	• DASF 5: Control access to data and other objects for metadata encryption across all data assets • DASF 8: Encrypt data at rest • DASF 9: Encrypt data in transit	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Attacker Gaining Control of Training Data Sources • Compromised AI System Integrity • Data Breach • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Response Costs • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	AML.T0051: LLM Prompt Injection AML.T0043: Craft Adversarial Data AML.T0053: LLM Plugin Compromise	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities  LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SC-8:Transmission Confidentiality and Integrity,  SC-12:Cryptographic Key Establishment and Management,  SC-28:Protection of Information at Rest,  AU-9:Protection of Audit Information,  SI-4:System Monitoring,  CM:Configuration Management Family,  PE:Physical and Environmental Protection Family	Encryption controls for protecting stored data and ensuring secure access.	no mapping	Apply a RBAC model, respecting the least privileged principle. Ensure ML applications comply with data security requirements. Ensure appropriate protection is deployed for test environments.	no mapping	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.24 Use of cryptography	Article 5.4: Prohibited AI Practices
Datasets 3.3	Data operations	Datasets 3.3: Label flipping	Label-flipping attacks are a distinctive type of data poisoning where the attacker manipulates the labels of a fraction of the training data. In these attacks, the attacker changes the labels of specific training points, which can mislead the ML model during training. Even with constrained capabilities, these attacks have been shown to significantly degrade the system’s performance, demonstrating their potential to compromise the accuracy and reliability of ML models.	DASF 5,DASF 8,DASF 9	DASF 5 DASF 8 DASF 9	• DASF 5: Control access to data and other objects for metadata encryption across all data assets • DASF 8: Encrypt data at rest • DASF 9: Encrypt data in transit	DASF v 1.0	Yes	No	No	No	No	No	• Degraded AI system Performance • Degraded AI System Reliability • Degraded AI System Accuracy • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement • Business Interruption Costs	AI Sec	AML.TA0011: Impact, disrupt availability or compromise integrity by manipulating business and operational processes	no mapping	no mapping	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement,  AC-6:Least Privilege,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  SI-4:System Monitoring, SI-7:Software, Firmware, and Information Integrity,  CM-2:Baseline Configuration,  CM-6:Configuration Settings	Integrity control mechanisms to protect against manipulation of labels in datasets.	no mapping	Control all data used by the ML model. Ensure reliable sources are used. Use methods to clean the training dataset from suspicious samples. Integrate poisoning control after the "model evaluation" phase. Apply a RBAC model, respecting the least privileged principle. Integrate ML specificities to awareness strategy and ensure all ML stakeholders are receiving it.	no mapping	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.24 Use of cryptography	no mapping
Governance 4.1	Data operations	Governance 4.1: Lack of traceability and transparency of model assets	The absence of traceability in data, model assets and models and the lack of accountable human oversight pose significant risks in machine learning systems. This lack of traceability can: Undermine the supportability and adoption of these systems, as it hampers the ability to maintain and update them effectively   Impact trust and transparency, which are essential for users to understand and rely on the system’s decisions  Limit the organization’s ability to meet regulatory, compliance and legal obligations, as these often require clear documentation and tracking of data and model-related processes	DASF 5,DASF 7,DASF 11,DASF 16,DASF 17,DASF 18,DASF 55	DASF 5 DASF 7 DASF 11 DASF 16 DASF 17 DASF 18 DASF 55	• DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 7: Enforce data quality checks on batch and streaming datasets for data sanity checks, and automatically detect anomalies before they make it to the datasets • DASF 11: Capture and view data lineage to capture the lineage all the way to the original raw data sources • DASF 16: Secure model features • DASF 17: Track and reproduce the training data used for ML model training and identify ML models and runs derived from a particular dataset • DASF 18: Govern model assets for traceability • DASF 55: Monitor audit logs	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Loss of Model Supportability • Loss of AI System Adoption • Limited Ability to Meet Regulatory Compliance • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Customer Retention • Regulatory Fines and Judgement	CyberSec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities  LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack, ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack  ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	CM-2:Baseline Configuration,  CM-3:Configuration Change Control,  CM-8:System Component Inventory,  AU-2:Event Logging,  AU-3:Content of Audit Records,  AU-6:Audit Record Review, Analysis, and Reporting,  SI-7:Software, Firmware, and Information Integrity,  SR-4:Provenance	Controls that ensure traceability and transparency for model and data assets, supporting governance needs.	no mapping	Ensure ML projects follow the global process for integrating security into projects. Apply documentation requirements to AI projects. Include ML applications into asset management processes. Build explainable models. Conduct a risk analysis of the ML application.	A.7.4 Quality of data for AI systems A.7.6 Data preparation A.4.3 Data resources A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.5 Data provenance A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.2 Privileged access rights A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Governance 4.2	Data operations	Governance 4.2: Lack of end-to-end ML lifecycle	Continuously measure, track and analyze key metrics, such as performance, accuracy and user engagement, to ensure the AI system’s reliability. Demonstrating consistent performance builds trustworthiness among users, customers and regulators.	DASF 19,DASF 21,DASF 42	DASF 19 DASF 21 DASF 42	• DASF 19: Manage end-to-end machine learning lifecycle for measuring, versioning, tracking model artifacts, metrics and results  • DASF 21: Monitor data and AI system from a single pane of glass • DASF 42: Data-centric MLOps and LLMOps unit and integration testing	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	Yes	• Inability to Demonstrate Performance • Inability to Demonstrate Trustworthiness • Regulatory non-compliance	• Financial Loss • Customer Retention • Regulatory Fines and Judgement	CyberSec	no mapping	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM10: Model Theft	ML08:2023 Model Skewing	CA-7:Continuous Monitoring,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  AU-11:Audit Record Retention,  SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation,  CM-2:Baseline Configuration,  CM-3:Configuration Change Control	Ensures integrity and auditing across the ML lifecycle for consistent, transparent management.	no mapping	Ensure ML projects follow the global process for integrating security into projects. Include ML applications into asset management processes. Conduct a risk analysis of the ML application. Integrate ML applications into the overall cyber-resilience strategy. Implement processes to maintain security levels of ML components over time.	A.6.1.2 Objectives for responsible development of AI system A.4.3 Data resources A.7.2 Data for development and enhancement of AI system A.7.4 Quality of data for AI systems A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.5 Data provenance A.7.6 Data preparation 	no mapping	no mapping
Algorithms 5.1	Model operations	Algorithms 5.1: Lack of tracking and reproducibility of experiments	ML development is often poorly documented and tracked, and results that cannot be reproduced may lead to overconfidence in an ML system’s performance. Common issues include:  Critical details missing from a model’s description   Results that are fragile, producing dramatically different results on a different GPU (even one that is supposed to be spec-identical)   Extensive tweaks to the authors’ system until it outperforms the untweaked “baseline,” resulting in asserted improvements that aren’t borne out in practice (particularly common in academic work)   Additionally, adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include the model itself, training data or its annotations, parts of the ML software stack, or even GPU hardware. In some instances, the attacker will need secondary access to fully carry out an attack using compromised supply chain components.	DASF 20,DASF 42,DASF 55,	DASF 20 DASF 42 DASF 55 	• DASF 20: Track ML training runs for documenting, measuring, versioning and tracking model artifacts including algorithms, training environment, hyperparameters, metrics and results • DASF 42: Data-centric MLOps and LLMOps promote models as code and automate ML tasks for cross-environment reproducibility • DASF 55: Monitor audit logs 	DASF v 1.0	Yes	No	No	No	No	No	• Overconfidence in ML Systems • Fragile and Differening Results when ML Systems are run on different GPU Hardware • Regulatory non-compliance	• Financial Loss • Response Costs • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM10: Model Theft	ML08:2023 Model Skewing  ML09:2023 Output Integrity Attack	CM-2:Baseline Configuration,  CM-3:Configuration Change Control SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting, 	Controls ensuring reproducibility through integrity mechanisms for data, model experiments, and tracking results.	no mapping	Ensure ML projects follow the global process for integrating security into projects. Apply documentation requirements to AI projects. Include ML applications into asset management processes.	A.6.2.4 AI system verification and validation A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Algorithms 5.2	Model operations	Algorithms 5.2: Model drift	Model drift in machine learning systems can occur due to changes in feature data or target dependencies. This drift can be broadly classified into three scenarios: Concept drift: where the statistical properties of the target variable change over time  Data drift: involving changes in the distribution of input data  Upstream data changes: occur due to alterations in data collection or processing methods before the data reaches the model  Clever attackers can exploit these scenarios to evade an ML system for adversarial purposes.	DASF 16,DASF 17,DASF 21	DASF 16 DASF 17 DASF 21	• DASF 16: Secure model features to track changes to features • DASF 17: Track training data with MLflow and Delta Lake to track upstream data changes  • DASF 21: Monitor data and AI system from a single pane of glass for changes and take action when changes occur. Have a feedback loop from a monitoring system and refresh models over time to help avoid model staleness.	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Inconsistent or Unexpected Model Predictions • Poor Performance and Reliability of Predictions • Erosion of Customer Trust • Adversarial Data Drift • Regulatory non-compliance	• Financial Loss • Response Costs • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	no mapping	ML02:2023 Data Poisoning Attack ML07:2023 Transfer Learning Attack	CM-2:Baseline Configuration,  CM-3:Configuration Change Control SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting, 	Focus on monitoring systems for ongoing changes, which supports detecting model drift.	no mapping	Define and monitor indicators for proper functioning of the model. Control all data used by the ML model. Ensure reliable sources are used. Ensure that the model is sufficiently resilient to the environment in which it will operate. Implement processes to maintain security levels of ML components over time.	A.4.3 Data resources A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.8.3 Information access restriction A.8.2 Privileged access rights	Article 72: Post-Market Monitoring by Providers and Post-Market Monitoring Plan for High-Risk AI Systems
Algorithms 5.3	Model operations	Algorithms 5.3: Hyperparameters stealing	Hyperparameters in machine learning are often deemed confidential due to their commercial value and role in proprietary learning processes. If attackers gain access to these hyperparameters, they may steal or manipulate them — altering, concealing or even adding hyperparameters. Such unauthorized interventions can harm the ML system, compromising performance and reliability or revealing sensitive algorithmic strategies.	DASF 20,DASF 42,DASF 43	DASF 20 DASF 42 DASF 43	• DASF 20: Track ML training runs in the model development process, including parameter settings, securely • DASF 42: Data-centric MLOps and LLMOps employing separate model lifecycle stages by UC schema • DASF 43: Use access control lists via workspace access controls 	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Breach of Confidential Proprietary Information or Trade Secrets • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Loss of Competitive Advantage • Proprietary Data Loss Liability	AI Sec	no mapping	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM10: Model Theft	ML08:2023 Model Skewing	AC-3:Access Enforcement,  AC-6:Least Privilege,  CM-2:Baseline Configuration,  CM-7:Least Functionality,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity	Ensures protection and encryption of sensitive model data, including hyperparameters.	Poisoning attacks -> Model poisoning	Reduce the available information about the model. Reduce the information given by the model.	A.6.2.4 AI system verification and validation "A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation"	A.5.15 Access control A.8.3 Information access restriction	no mapping
Algorithms 5.4	Model operations	Algorithms 5.4: Malicious libraries	Attackers can upload malicious libraries to public repositories that have the potential to compromise systems, data and models. Administrators should manage and restrict the installation and usage of third-party libraries, safeguarding systems, pipelines and data. This risk may also manifest in 2.2 Data Prep in exploratory data analysis (EDA).	DASF 53	DASF 53	• DASF 53: Third-party library control to limit the potential for malicious third-party libraries and code to be used on mission-critical workloads	DASF v 1.0	Yes	No	No	Yes	No	No	• Supply Chain Attack • Cyber Security Incident • Loss of AI System Availability • Regulatory Non-compliance	• Financial Loss • Legal Fines • Response Costs • Customer Retention • Regulatory Fines and Judgement	CyberSec	AML.T0017.000 Adversarial ML Attacks	no mapping	no mapping	no mapping	SR-3:Supply Chain Controls and Processes,  CM-2:Baseline Configuration,  CM-10:Software Usage Restrictions,  SI-3:Malicious Code Protection,  SI-4:System Monitoring	Controls managing software integrity, preventing introduction of malicious libraries in ML processes.	Supply chain attacks -> Compromised 3rd-party models or code 	Check the vulnerabilities of the components used so that they have an appropriate security level. Ensure ML projects follow the global process for integrating security into projects.	A.4.4 Tooling resources A.6.2.3 Documentation of AI system design and development	A.8.25 Secure development life cycle	Article 15.5: Accuracy, Robustness and Cybersecurity
Evaluation 6.1	Model operations	Evaluation 6.1: Evaluation data poisoning	Upstream attacks against data, where the data is tampered with before it is used for machine learning, significantly complicate the training and evaluation of ML models. Poisoning of the evaluation data impacts the model validation and testing process. These attacks can corrupt or alter the data in a way that skews the training process, leading to unreliable models.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 7,DASF 11,DASF 42,DASF 44,DASF 45,DASF 49	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 7 DASF 11 DASF 42 DASF 44 DASF 45 DASF 49	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists to restrict the IP addresses that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 7: Enforce data quality checks on batch and streaming datasets for data sanity checks, and automatically detect anomalies before they make it to the datasets • DASF 11: Capture and view data lineage to capture the lineage all the way to the original raw data sources • DASF 42: Data-centric MLOps and LLMOps unit and integration testing • DASF 44: Trigger actions in response to a specific event via automated jobs to notify human-in-the-loop (HITL)  • DASF 45: Evaluate models to capture performance insights for language models • DASF 49: Automate LLM evaluation	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Unreliable Model Training Processes and Outcomes • Loss of AI System Integrity • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Model Retraining Costs	AI Sec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.1: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	MP-2:Media Access,  MP-4:Media Storage,  SC-28:Protection of Information at Rest,  SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation	Integrity mechanisms protect against evaluation data tampering, ensuring model reliability.	Poisoning attacks -> Data poisoning	Ensure ML applications comply with data security requirements. Ensure reliable sources are used. Define and monitor indicators for proper functioning of the model.	A.4.3 Data resources A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.6.2.6 AI system operation and monitoring A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development A.6.2.4 AI system verification and validation A.9.3 Objectives for responsible use of AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.16 Monitoring Activities	no mapping
Evaluation 6.2	Model operations	Evaluation 6.2: Insufficient evaluation data	Evaluation datasets can also be too small or too similar to the training data to be useful. Poor evaluation data can lead to biases, hallucinations and toxic output. It is difficult to effectively evaluate large language models (LLMs), as these models rarely have an objective ground truth labeled. Consequently, organizations frequently struggle to determine the trustworthiness of these models in critical, unsupervised use cases, given the uncertainties in their evaluation.	DASF 22,DASF 25,DASF 47,DASF 45	DASF 22 DASF 25 DASF 47 DASF 45	• DASF 22: Build models with all representative, accurate and relevant data sources to evaluate on clean and sufficient data • DASF 25: Use retrieval augmented generation (RAG) with large language models (LLMs) • DASF 47: Compare LLM outputs on set prompts to assess LLM project with an interactive prompt interface • DASF 45: Evaluate models to capture performance insights for language models	DASF v 1.0	Yes	No	Yes	Yes	No	Yes	• Reduced Organisation Trust in Model Effectiveness and Performance • Introduced Biases • Introduced Hallucinations • Introduced Toxic Outputs • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilitie	no mapping	no mapping	Ensures monitoring and auditability of evaluation data, with a focus on model trustworthiness.	no mapping	Ensure ML applications comply with data security requirements. Control all data used by the ML model. Use methods to clean the training dataset from suspicious samples. Conduct a risk analysis of the ML application	A.6.2.4 AI system verification and validation A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system A.4.3 Data resources A.6.2.4 AI system verification and validation A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development 	no mapping	no mapping
Evaluation 6.3	Model operations	Evaluation 6.3: Lack of Interpretability and Explainability	A lack of interpretability and explainability poses a unique challenge due to the scale and complexity of frontier or GenAI LLMs. The degree to which we can understand the internal workings of a model and trace its decision-making process refers to interpretability. The ability to provide a human-understandable explanation of a model's outputs or decision-making process refers to explainability. Both interpretability and explainability are required to develop trustworthiness in AI systems and are also required to assist with understanding and mitigating security risks such as Prompt Injection, Model Inversion, Misuse, Data Breach, Data Poisoning, Adversarial Attacks, Security Auditing and Regulatory Compliance. References:  Mechanistic Interpretability for AI Safety -- A Review Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks	DASF 35,DASF 37,DASF 42,DASF 45	DASF 35 DASF 37 DASF 42 DASF 45	• DASF 35: Track model performance to evaluate quality • DASF 37: Set up inference tables for monitoring and debugging prompts • DASF 42: Data-centric MLOps and LLMOps promote models as code • DASF 45: Evaluate models to capture performance insights for language models  Additional controls for consideration:  - Continuous benchmark testing of features throughout the model lifecycle in prediction accuracy, traceability and decision understanding. LIME, SHAP, DEEPLIFT and dashboards of factors influencing decisions.  - Isolate functionality using a modular architecture.   - Implement Explainable AI tools and techniques that can generate human-understandable explanations for model outputs.  - Model Cards, or a data sheet that provides information on data training sets, known biases, evaluation tests, intended uses, and unintended uses. From the perspective of how the AI Developer trained the model or how my organization has modified the model if my organization has fine-tuned that model.  - Red Teaming and Adversarial Machine Learning  - Human in the loop or Human at the helm.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 2.0	Yes	Yes	Yes	Yes	Yes	Yes	• Lack of Reasoning for Model Outputs and Decisions • Compromise of AI Systems • Subsequent Downstream System Impact or Compromise	• Regulatory Non-compliance • Legal Fines • Response Costs • Customer Retention	AI Sec	no mapping	no mapping	no mapping	no mapping	no mapping		no mapping	Build explainable models.	A.6.2.4 AI system verification and validation A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development 	no mapping	Article 10.5: Data and Data Governance
Model 7.1	Model operations	Model 7.1: Backdoor machine learning / Trojaned model	There are inherent risks when using public ML/LLM models or outsourcing their training, akin to the dangers associated with executable (.exe) files. A malicious third party handling the training process could tamper with the data or deliver a “Trojan model” that intentionally misclassifies specific inputs. Additionally, open source models may contain hidden malicious code that can exfiltrate sensitive data upon deployment. These risks are pertinent in both external models and outsourced model development scenarios, necessitating scrutiny and verification of models before use.	DASF 1,DASF 5,DASF 19,DASF 23,DASF 34,DASF 42,DASF 43,DASF 55,DASF 56	DASF 1 DASF 5 DASF 19 DASF 23 DASF 34 DASF 42 DASF 43 DASF 55 DASF 56	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 5: Control access to data and other objects  • DASF 19: Manage end-to-end machine learning lifecycle • DASF 23: Register, version, approve, promote and deploy models and scan models for malicious code when using third-party models or libraries • DASF 34: Run models in multiple layers of isolation. Models are considered untrusted code: deploy models and custom LLMs with multiple layers of isolation.  • DASF 42: Data-centric MLOps and LLMOps promote models as code using CI/CD. Scan third-party models continuously to identify hidden cybersecurity risks and threats such as malware, vulnerabilities and integrity issues to detect possible signs of malicious activity, including malware, tampering and backdoors. See resources section for third-party tools. • DASF 43: Use access control lists to limit who can bring models and limit the use of public models • DASF 55: Monitor audit logs • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses	DASF v 1.0	Yes	No	Yes	No	No	Yes	• Unknown Compromises within AI System Models • Uncontrolled AI System Models • Hidden Malicious Code within AI System Models • Breach of Sensitive Information • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Legal Fines • Response Costs • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.TA0006: Persistence AML.T0016.000: Adversarial ML Attack Implementations AML.T0018: Backdoor ML Model	ID: T1587.001 Develop Capabilities: Malware	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack  ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	CM-7:Least Functionality,  CM-11:User-Installed Software,  SA-10:Developer Configuration Management,  SA-11:Developer Testing and Evaluation,  SC-28:Protection of Information at Rest,  SC-39:Process Isolation,  CA-7:Continuous Monitoring,  SI-4:System Monitoring	Controls protecting against trojaned models through security and data integrity mechanisms.	Poisoning attacks -> Model poisoning	Enlarge the training dataset. Apply a RBAC model, respecting the least privilege principle Ensure ML applications comply with data security requirements. Control all data used by the ML model. Ensure reliable sources are used. Use methods to clean the training dataset from suspicious samples. Integrate poisoning control after the "model evaluation" phase. Integrate ML specificities to awareness strategy and ensure all ML stakeholders are receiving it.	A.6.1.2 Objectives for responsible development of AI system A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.4.3 Data resources A.7.2 Data for development and enhancement of AI system A.7.4 Quality of data for AI systems A.6.2.5 AI system deployment A.6.2.7 AI system technical documentation A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation	A.5.16 Identity management A.8.5 Secure authentication A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.22 Segregation of networks A.8.15 Logging A.8.20 Networks Security A.8.23 Web filtering A.8.16 Monitoring Activities	Article 15.5: Accuracy, Robustness and Cybersecurity
Model 7.2	Model operations	Model 7.2: Model assets leak	Adversaries may target ML artifacts for exfiltration or as a basis for staging ML attacks. These artifacts encompass models, datasets and metadata generated during interactions with a model. Additionally, insiders risk leaking critical model assets like notebooks, features, model files, plots and metrics. Such leaks can expose trade secrets and sensitive organizational information, underlining the need for stringent security measures to protect these valuable assets.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 28,DASF 30,DASF 31,DASF 32,DASF 37	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 28 DASF 30 DASF 31 DASF 32 DASF 37	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data  to restrict IP addresses  • DASF 3: IP access lists to restrict the IP addresses that can authenticate to Databricks  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Unity Catalog privileges and securable objects for permissions model across all data assets to protect data and sources  • DASF 24: Protect model assets, lifecycle and security with UC in MLflow Model Registry  • DASF 28: Create and model aliases, tags and annotations in Unity Catalog for documenting and discovering models • DASF 30: Encrypt models • DASF 31: Secure serving endpoint with Model Serving • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. The most reliable mitigation is to always treat all LLM productions as potentially malicious and under the control of any entity that has been able to inject text into the LLM user’s input. Implement gates between users/callers and the actual model by performing input validation on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. • DASF 37: Set up inference tables for monitoring and debugging models	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Subsequence Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.TA0010: Exfiltration AML.T0007: Discover ML Artifacts AML.T0035: ML Artifact Collection	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	MP-2:Media Access,  MP-4:Media Storage,  SC-28:Protection of Information at Rest,  AC-3:Access Enforcement,  AC-6:Least Privilege,   AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  SI-4:System Monitoring,  PS-6:Access Agreements	Access and least privilege controls to protect sensitive model assets from unauthorized access or leaks.	Input-based attacks -> Model extraction and theft	Apply a RBAC Model, Respecting the Least Privilege Principle. Ensure ML Applications Comply with Data Security Requirements. Ensure Appropriate Protection is Deployed for Test Environments. Conduct a Risk Analysis of the ML Application. Control All Data Used by the ML Model. Reduce the Information Given by the Model. Reduce the Available Information About the Model. Use Federated Learning to Minimize the Risk of Data Breaches.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model 7.3	Model operations	Model 7.3: ML Supply chain vulnerabilities	Due to the extensive data, skills and computational resources required to train machine learning algorithms, it’s common practice to reuse and slightly modify models developed by large corporations. For example, ResNet, a popular image recognition model from Microsoft, is often adapted for customer-specific tasks. These models are curated in a Model Zoo (Caffe hosts popular image recognition models) or hosted by third-party ML SaaS (OpenAI LLMs are an example). In this attack, the adversary attacks the models hosted in Caffe, thereby poisoning the well for anyone else. Adversaries can also host specialized models that will receive less scrutiny, akin to watering hole attacks.	DASF 22,DASF 27,DASF 42,DASF 45,DASF 48,DASF 53,DASF 56	DASF 22 DASF 27 DASF 42 DASF 45 DASF 48 DASF 53 DASF 56	• DASF 22: Build models with all representative, accurate and relevant data sources to minimize third-party dependencies for models and data where possible • DASF 27: Pretrain a large language model (LLM) on your own IP • DASF 42: Data-centric MLOps and LLMOps promote models as code using CI/CD.  Scan third-party models continuously to identify hidden cybersecurity risks and threats such as malware, vulnerabilities and integrity issues to detect possible signs of malicious activity, including malware, tampering and backdoors. See resources section for third-party tools.  • DASF 45: Evaluate models and validate (aka, stress testing) to verify reported function and disclosed weaknesses in the models • DASF 48: Use hardened runtime for machine learning  • DASF 53: Third-party library control • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses	DASF v 1.0	Yes	No	Yes	Yes	No	Yes	• Compromise of AI System • Uncontrolled AI System Models • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.TA0003: Resource Development - creating, purchasing, or compromising/stealing resources AML.T0002: Acquire Public ML Artifacts AML.T0017.000: Adversarial ML Attacks AML.T0010: ML Supply Chain Compromise AML.T0010.003: ML Supply Chain Model Compromise AML.T0011.000: Unsafe ML Artifacts	ID: T1554 Compromise Host Software Binary ID: T1195 Supply Chain Compromise	LLM01: Prompt Injection LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM10: Model Theft	ML08:2023 Model Skewing	SR-2:Supply Chain Risk Management Plan,  SR-3:Supply Chain Controls and Processes,  SR-6:Supplier Assessments and Reviews,  SR-10:Inspection of Systems or Components,  SR-11:Component Authenticity,  SA-4:Acquisition Process,  SA-8:Security and Privacy Engineering Principles,  RA-3:Risk Assessment	Protects against risks in the supply chain through boundary protection and integrity monitoring.	Supply chain attacks -> Compromised 3rd-party models or code	Ensure ML Applications Comply with Security Policies. Ensure ML Applications Comply with Third Parties' Security Requirements. Include ML Applications in Asset Management Processes. Check the Vulnerabilities of the Components Used so that they Have an Appropriate Security Level. Conduct a Risk Analysis of the ML Application. Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Implement Processes to Maintain Security Levels of ML Components over Time. Reduce the Available Information About the Model.	A.6.2.4 AI system verification and validation A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system A.4.3 Data resources A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible design and development of AI systems A.6.2.3 Documentation of AI system design and development A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.4.4 Tooling resources	A.8.9 Configuration management A.8.25 Secure development life cycle A.8.15 Logging A.8.16 Monitoring Activities 	Article 15.5: Accuracy, Robustness and Cybersecurity
Model 7.4	Model operations	Model 7.4: Source code control attack	The attacker might modify the source code used in the ML algorithm, such as the random number generator or any third-party libraries, which are often open source.	DASF 52,DASF 53,DASF 56	DASF 52 DASF 53 DASF 56	• DASF 52: Source code control to control and audit your knowledge object integrity • DASF 53: Third-party library control for third-party library integrity • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Subsequence Compromise of AI Systems • Loss of AI System Integrity • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	CyberSec	AML.T0016.001: Software Tools	ID: T1554 Compromise Host Software Binary ID: T1195 Supply Chain Compromise	LLM01: Prompt Injection LLM03.4, 5: Training Data Poisoning LLM06.3: Sensitive Information Disclosure LLM10.2: Model Theft 	no mapping	no mapping	Secure SDLC practices to ensure source code integrity and mitigate vulnerabilities.	Supply chain attacks -> Compromised 3rd-party models or code	Apply a RBAC Model, Respecting the Least Privilege Principle. Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Check the Vulnerabilities of the Components Used so that they Have an Appropriate Security Level. Conduct a Risk Analysis of the ML Application.	A.4.4 Tooling resources A.6.2.3 Documentation of AI system design and development	A.8.25 Secure development life cycle A.8.20 Networks Security A.8.23 Web filtering	Article 15.5: Accuracy, Robustness and Cybersecurity
Model Management 8.1	Model operations	Model Management 8.1: Model attribution	Inadequate governance in machine learning, including a lack of robust access controls, unclear model classification and insufficient documentation, can lead to the improper use or sharing of models. This risk is particularly acute when transferring models outside their designed purpose. To mitigate these risks, groups that post models must provide precise descriptions of their intended use and document how they address potential risks.	DASF 5,DASF 28,DASF 29,DASF 51	DASF 5 DASF 28 DASF 29 DASF 51	• DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 28: Create model aliases, tags and annotations for documenting and discovering models • DASF 29: Build MLOps workflows with human-in-the-loop (HITL) , model stage management and approvals • DASF 51: Share data and AI assets securely	DASF v 1.0	Yes	No	Yes	Yes	Yes	Yes	• Unauthorised Sharing of Models • Unintended Use of Models • Inappropriate Use of Models • Regulatory Non-compliance	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.5, 7: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML06:2023 AI Supply Chain Attacks ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management,  AC-6:Least Privilege,  AC-17:Remote Access,  CM-2:Baseline Configuration,  CM-7:Least Functionality,  RA-3:Risk Assessment,  RA-5:Vulnerability Monitoring and Scanning,  SA-4:Acquisition Process,  SA-8:Security and Privacy Engineering Principles,   AT:Awareness and Training Family,  CM-8:System Component Inventory	Ensures auditability and attribution of models through access controls and logging.	no mapping	Apply a RBAC Model, Respecting the Least Privilege Principle. Ensure ML Applications Comply with Security Policies. Ensure ML Applications Comply with Third Parties’ Security Requirements. Assess the Regulations and Laws the ML Application Must Comply With. Reduce the Available Information About the Model. Reduce the Information Given by the Model. Ensure Appropriate Protection is Deployed for Test Environments.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.6.2.5 AI system deployment	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.5.14 Information transfer	no mapping
Model Management 8.2	Model operations	Model Management 8.2: Model theft	Training machine learning systems, particularly large language models, involves considerable investment. A significant risk is the potential theft of a system’s knowledge through direct observation of their input and output observations, akin to reverse engineering. This can lead to unauthorized access, copying or exfiltration of proprietary models, resulting in economic losses, eroded competitive advantage and exposure of sensitive information. This attack can be as simple as attackers making legitimate queries and analyzing the responses to recreate a model. Once replicated, the model can be inverted, enabling the attackers to extract feature information or infer details about the training data.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 30,DASF 31,DASF 51,DASF 32,DASF 33,DASF 59,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 30 DASF 31 DASF 51 DASF 32 DASF 33 DASF 59 DASF 55 	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints to prevent access and compute theft • DASF 51: Share data and AI assets securely • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit APIs • DASF 33: Manage credentials securely to prevent credentials of data sources used for model training from leaking through models • DASF 59: Use clean rooms to collaborate in a secure environment • DASF 55: Monitor audit logs 	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Breach of Confidential Proprietary Information or Trade Secrets • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Loss of Competitive Advantage	AI Sec	AML.TA0000: ML Model Access AML.T0002.001: Acquire Public ML Artifacts: Models AML.T0044: Full ML Model Access AML.T0048.004: ML Intellectual Property Theft	ID: T1134.001 Token Impersonation/Theft  ID: T1087 Account Discovery ID: T1098 Account Manipulation ID: T1110 Brute Force ID: T1528 Steal Application Access Token	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning,  LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management,  AC-6:Least Privilege,   AU-9:Protection of Audit Information,  SI-15:Information Output Filtering,  SI-19:De-identification,  PL-8:Security and Privacy Architectures,  SA-8:Security and Privacy Engineering Principles	Protects against model theft through secure access control and encryption.	Input-based attacks -> Model extraction and theft	Apply a RBAC Model, Respecting the Least Privilege Principle. Ensure ML Applications Comply with Identity Management, Authentication, and Access Control Policies. Ensure ML Applications Comply with Security Policies. Ensure ML Applications Comply with Third Parties’ Security Requirements. Reduce the Available Information About the Model. Ensure Appropriate Protection is Deployed for Test Environments. Use Federated Learning to Minimize the Risk of Data Breaches.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.5.14 Information transfer A.8.5 Secure Authentication A.5.34 Privacy and protection of personal identifiable information (PII) A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Model Management 8.3	Model operations	Model Management 8.3: Model lifecycle without HITL (human-in-the-loop)	Lack of sufficient controls in a machine learning and systems development lifecycle can result in the unintended deployment of incorrect or unapproved models to production. Implementing model lifecycle tracking within an MLOps framework is advisable to mitigate this risk. This approach should include human oversight, ensuring permissions, version control and proper approvals are in place before models are promoted to production. Such measures are crucial for maintaining ML system integrity, reliability and security.	DASF 5,DASF 24,DASF 28,DASF 29,DASF 42	DASF 5 DASF 24 DASF 28 DASF 29 DASF 42	• DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 28: Create model aliases, tags and annotations • DASF 29: Build MLOps workflows with human-in-the-loop (HILP) with permissions, versions and approvals to promote models to production • DASF 42: Data-centric MLOps and LLMOps promote models as code using CI/CD	DASF v 1.0	Yes	Yes	No	Yes	No	No	• Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM05.5, 7: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.1,4: Model Theft	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML06:2023 AI Supply Chain Attacks ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation,  RA-3:Risk Assessment,  CM-3:Configuration Change Control,  AC-6:Least Privilege,  SI-4:System Monitoring,  CA-7:Continuous Monitoring,  SA-3:System Development Life Cycle,  SA-11:Developer Testing and Evaluation,  SR-3:Supply Chain Controls and Processes	Ensures integrity and auditability across the model lifecycle, including human oversight.	no mapping	Apply Documentation Requirements to AI Projects. Assess the Regulations and Laws the ML Application Must Comply With. Ensure ML Applications Comply with Security Policies. Ensure ML Projects Follow the Global Process for Integrating Security Into Projects. Integrate ML Applications Into the Overall Cyber-Resilience Strategy. Conduct a Risk Analysis of the ML Application. Build Explainable Models. Define and Monitor Indicators for Proper Functioning of the Model. Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving it.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.6.2.5 AI system deployment A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation	A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code	Article 14.2: Human Oversight
Model Management 8.4	Model operations	Model Management 8.4: Model inversion	In machine learning models, private assets like training data, features and hyperparameters, which are typically confidential, can potentially be recovered by attackers through a process known as model inversion. This technique involves reconstructing private elements without direct access, compromising the model’s security. Model inversion falls under the “Functional Extraction” category in the MITRE ATLAS framework, highlighting its relevance as a significant security threat.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 30,DASF 31,DASF 32,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 30 DASF 31 DASF 32 DASF 55	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit APIs • DASF 55: Monitor audit logs	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Breach of Confidential Proprietary Information or Trade Secrets • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Loss of Competitive Advantage • Proprietary Data Loss Liability	AI Sec	AML.T0024.001: Invert ML Model AML.T0024.002: Extract ML Model	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-12:Information Management and Retention,  SI-15:Information Output Filtering,  SI-19:De-identification, PL-8:Security and Privacy Architectures,  AC-2:Account Management,  SA-11:Developer Testing and Evaluation	Protects against adversarial model inversion through integrity mechanisms and access controls.	Input-based attacks - > Model inversion	Apply a RBAC Model, Respecting the Least Privilege Principle. Reduce the Available Information About the Model. Reduce the Information Given by the Model. Ensure that Models Respect Differential Privacy to a Sufficient Degree. Use Federated Learning to Minimize the Risk of Data Breaches.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference requests 9.1	Model deployment and serving	Model Serving — Inference requests 9.1: Prompt inject	A direct prompt injection occurs when a user injects text that is intended to alter the behavior of the LLM. Malicious input, known as model evasion in the MITRE ATLAS framework, is a significant threat to machine learning systems. These risks manifest as “adversarial examples”: inputs deliberately designed to deceive models. Attackers use direct prompt injections to bypass safeguards in order to create misinformation and cause reputational damage. Attackers may wish to extract the system prompt or reveal private information provided to the model in the context but not intended for unfiltered access by the user. Large language model (LLM) plug-ins are particularly vulnerable, as they are typically required to handle untrusted input and it is difficult to apply adequate application control. Attackers can exploit such vulnerabilities, with severe potential outcomes including remote code execution.	DASF 1,DASF 3,DASF 4,DASF 5,DASF 24,DASF 46,DASF 30,DASF 31,DASF 32,DASF 37,DASF 54,DASF 56,DASF 60	DASF 1 DASF 3 DASF 4 DASF 5 DASF 24 DASF 46 DASF 30 DASF 31 DASF 32 DASF 37 DASF 54 DASF 56 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platformDASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful.  • DASF 37: Set up inference tables for monitoring and debugging prompts  • DASF 54: Implement LLM guardrails  • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses  • DASF 60: Rate limit number of inference queries  Additional controls to consider:  Robust Intelligence AI Firewall Prompt Injection rule: Flags malicious user input that might direct the LLM to perform an action unintended by the model creator. HiddenLayer AISec SafeLLM Proxy.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Unauthorised Alteration of AI System Outputs • Unauthorised Access to Private Information • Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec	AML.T0051: LLM Prompt Injection	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-4:System Monitoring,  SI-10:Information Input Validation,  AC-6:Least Privilege,  SA-11:Developer Testing and Evaluation	Controls protecting against adversarial examples through input validation and integrity mechanisms.	Input-based attacks - > Prompt injection	no mapping	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.20 Networks Security A.8.23 Web filtering	no mapping
Model Serving — Inference requests 9.2	Model deployment and serving	Model Serving — Inference requests 9.2: Model inversion	Malicious actors can recover the private assets used in machine learning models, known as functional extraction in the MITRE ATLAS framework. This process includes reconstructing private training data, features and hyperparameters the attacker cannot otherwise access. The attacker can also recover a functionally equivalent model by iteratively querying the model.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 46,DASF 30,DASF 31,DASF 32,DASF 37,DASF 54,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 46 DASF 30 DASF 31 DASF 32 DASF 37 DASF 54 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. Open source and commercial solutions provide a variety of modules including prompt and output scanners for various responsible AI or jailbreaking attacks.  • DASF 37: Set up inference tables for monitoring and debugging model prompts  • DASF 54: Implement AI guardrails  • DASF 60: Rate limit number of inference queries  Additional controls to consider:  Robust Intelligence AI Firewall Prompt Injection rule: Flags malicious user input that might direct the LLM to perform an action unintended by the model creator.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Breach of Confidential Proprietary Information or Trade Secrets • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Loss of Competitive Advantage • Proprietary Data Loss Liability	AI Sec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-12:Information Management and Retention,  SI-15:Information Output Filtering,  SI-19:De-identification, PL-8:Security and Privacy Architectures,  AC-2:Account Management,  SA-11:Developer Testing and Evaluation 	Protects against model inversion through secure access control and model encryption.	Input-based attacks - > Model inversion	Reduce the Available Information About the Model. Reduce the Information Given by the Model. Ensure that Models Respect Differential Privacy to a Sufficient Degree.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography 	no mapping
Model Serving — Inference requests 9.3	Model deployment and serving	Model Serving — Inference requests 9.3: Model breakout	Malicious users can exploit adversarial examples to mislead machine learning systems, including large language models (LLMs). These specially crafted inputs aim to disrupt the normal functioning of these systems, leading to several potential hazards. An attacker might use these examples to force the system to deviate from its intended environment, exfiltrate sensitive data or interact inappropriately with other systems. Additionally, adversarial inputs can cause false predictions, leak sensitive information from the training data, or manipulate the system into executing unintended actions on internal and external systems.	DASF 34,DASF 37,DASF 56	DASF 34 DASF 37 DASF 56	• DASF 34: Run models in multiple layers of isolation with unprivileged VMs and network segregation. Protects back-end internal systems from LLM access. The most reliable mitigation is to always treat all LLM output as potentially malicious and remember that an untrusted entity has been able to inject text as user input. All LLM output should be inspected and sanitized before being further parsed to extract information related to the plug-in. Plug-in templates should be parameterized wherever possible, and any calls to external services must be strictly parameterized at all times and made in a least-privileged context.  • DASF 37: Set up inference tables for monitoring and debugging prompts • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Loss of AI System Availability • Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention • Regulatory Non-compliance • Direct Business Interruption	AI Sec	AML.TA0005: Execution: adversary-controlled code running on a local or remote system AML.T0024: Exfiltration via ML Inference API	no mapping	LLM01: Prompt Injection LLM03.4, 5: Training Data Poisoning LLM06.3: Sensitive Information Disclosure LLM10.2: Model Theft 	ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SI-4:System Monitoring,  SI-10:Information Input Validation,  SA-11:Developer Testing and Evaluation,  CA-8:Penetration Testing,  PL-8:Security and Privacy Architectures	Secures model serving environments against breakout attacks through integrity and isolation controls.	Input-based attacks -> Evasion (including adversarial examples)  Input-based attacks - > Prompt injection	Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving It. Implement Tools to Detect if a Data Point is an Adversarial Example or Not. Include ML Applications in Detection and Response to Security Incident Processes. Add some Adversarial Examples to the Training Dataset. Apply Modifications on Inputs. Choose and Define a More Resilient Model Design. Reduce the Available Information About the Model. Reduce the Information Given by the Model. Use Less Easily Transferable Models.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.8.22 Segregation of networks A.8.20 Networks Security A.8.23 Web filtering	no mapping
Model Serving — Inference requests 9.4	Model deployment and serving	Model Serving — Inference requests 9.4: Looped input	There is a notable risk in machine learning systems when the output produced by the system is reintroduced into the real world and subsequently cycles back as input, creating a harmful feedback loop. This can reinforce removing security filters, biases or errors, potentially leading to increasingly skewed or inaccurate model performance and unintended system behaviors.	DASF 37	DASF 37	• DASF 37: Set up inference tables for monitoring and debugging models to capture incoming requests and outgoing responses to your model serving endpoint and automatically log them in tables. Afterward, you can use the data in this table to monitor, debug and improve ML models and decide if these inferences are of sufficient quality for input to model training.	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec		no mapping	no mapping	no mapping	CA-7:Continuous Monitoring,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity, CM-2:Baseline Configuration,  CM-3:Configuration Change Control,  CM-9:Configuration Management Plan,  RA-3:Risk Assessment 	Protects against feedback loops through logging and input validation mechanisms.		Define and Monitor Indicators for Proper Functioning of the Model. Ensure that the Model is Sufficiently Resilient to the Environment in Which it Will Operate.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	no mapping	no mapping
Model Serving — Inference requests 9.5	Model deployment and serving	Model Serving — Inference requests 9.5: Infer training data membership	Adversaries may pose a significant privacy threat to machine learning systems by simulating or inferring whether specific data samples were part of a model’s training set. Such inferences can be made by: Using techniques like Train Proxy via Replication to create and host shadow models replicating the target model’s behavior  Analyzing the statistical patterns in the model’s prediction scores to conclude the training data   These methods can lead to the unintended leakage of sensitive information, such as individuals’ personally identifiable information (PII) in the training dataset or other forms of protected intellectual property.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 28,DASF 46,DASF 30,DASF 31,DASF 32,DASF 37,DASF 45,DASF 54,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 28 DASF 46 DASF 30 DASF 31 DASF 32 DASF 37 DASF 45 DASF 54 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 28: Create model aliases, tags and annotations • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful.  • DASF 37: Set up inference tables for monitoring and debugging models • DASF 45: Evaluate models for custom evaluation metrics  • DASF 54: Implement AI guardrails  • DASF 60: Rate limit number of inference queries  Additional controls to consider:  Robust Intelligence AI Firewall Prompt Injection rule: Flags malicious user input that might direct the LLM to perform an action unintended by the model creator.  Robust Intelligence AI Firewall PII Detection rule: Flags user input and model output suspected of containing PII.  The HiddenLayer AISec Platform, specifically MLDR, monitors inputs and related outputs to ML models to determine if an adversary is attempting an inference with a malicious intent.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Breach of Confidential Proprietary Information or Trade Secrets • Breach of Personal Information • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Loss of Competitive Advantage • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec	AML.T0016: Obtain Capabilities AML.T0017: Develop Capabilities AML.T0024.000: Infer Training Data Membership	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement,  AC-6:Least Privilege,    SC-7:Boundary Protection,  SC-8:Transmission Confidentiality and Integrity,   SC-28:Protection of Information at Rest	Prevents data inference attacks through strict data integrity and access controls.	Input-based attacks - > Model inversion	Apply a RBAC model, respecting the least privileged principle. Ensure that Models Respect Differential Privacy to a Sufficient Degree. Reduce the Available Information About the Model. Reduce the Information Given by the Model. Use Federated Learning to Minimize Risk of Data Breaches.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.4.4 Tooling resources A.6.2.8 AI system recording of event logs A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development A.6.2.4 AI system verification and validation A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference requests 9.6	Model deployment and serving	Model Serving — Inference requests 9.6: Discover ML model ontology	Adversaries may aim to uncover the ontology of a machine learning model’s output space, such as identifying the range of objects or responses the model is designed to detect. This can be achieved through repeated queries to the model, which may force it to reveal its classification system or by accessing its configuration files or documentation. Understanding a model’s ontology allows adversaries to gain insights in designing targeted attacks that exploit specific vulnerabilities or characteristics.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 28,DASF 46,DASF 30,DASF 31,DASF 32,DASF 37,DASF 45,DASF 54,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 28 DASF 46 DASF 30 DASF 31 DASF 32 DASF 37 DASF 45 DASF 54 DASF 60 	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 28: Create model aliases, tags and annotations • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. Open source and commercial solutions provide a variety of modules including prompt and output scanners for various responsible AI or jailbreaking attacks.  • DASF 37: Set up inference tables for monitoring and debugging models • DASF 45: Evaluate models for custom evaluation metrics • DASF 54: Implement AI guardrails  • DASF 60: Rate limit number of inference queries 	DASF v 1.0	Yes	No	No	No	No	No	• Subsequence Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.T0013: Discover ML Model Ontology	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement,  AC-6:Least Privilege,    AU-2:Event Logging,  SI-10:Information Input Validation	Protects sensitive model data, preventing adversaries from discovering model ontologies.	Input-based attacks -> Model extraction and theft	Reduce the Available Information About the Model. Reduce the Information Given by the Model. Apply Modifications on Inputs. Choose and Define a More Resilient Model Design.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.4.4 Tooling resources A.6.2.8 AI system recording of event logs A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development A.6.2.4 AI system verification and validation A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference requests 9.7	Model deployment and serving	Model Serving — Inference requests 9.7: Denial of Service (DoS)	Adversaries may target machine learning systems with a flood of requests to degrade or shut down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 46,DASF 30,DASF 31,DASF 32,DASF 37,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 46 DASF 30 DASF 31 DASF 32 DASF 37 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. • DASF 37: Set up inference tables for monitoring and debugging model prompts • DASF 60: Rate limit number of inference queries  Additional controls for consideration:  - Robust Intelligence AI Firewall Prompt Injection rule: Flags malicious user input that might direct the LLM to perform an action unintended by the model creator.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	Yes	No	Yes	Yes	Yes	Yes	• Loss of AI System Availability • Disruption of Business Operations and Service Availability	• Financial Loss • Breach of Service Level Agreement • Direct Business Interruption • Customer Retention	CyberSec	AML.TA0011: Impact: manipulate, interrupt, erode confidence in, or destroy your machine learning systems and data AML.T0029: Denial of ML Service ID: T1499 Endpoint Denial of Service	ID: T1499 Endpoint Denial of Service	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack  ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SC-5:Denial-of-Service Protection,  SC-6:Resource Availability,  SC-7:Boundary Protection,  SI-4:System Monitoring, SI-10:Information Input Validation,  CA-7:Continuous Monitoring	Boundary protection and monitoring controls that mitigate DoS attacks on ML systems.	Availability attacks -> Denial of AI service  	Control all data used by the ML model. Implement tools to detect if a data point is an adversarial example or not. Apply a RBAC model, respecting the least privileged principle. Ensure ML applications comply with protection policies and are integrated to security operations processes. Include ML applications into detection and response to security incident processes. Integrate ML applications into the overall cyber-resilience strategy.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development A.6.2.4 AI system verification and validation 	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference requests 9.8	Model deployment and serving	Model Serving — Inference requests 9.8: LLM hallucinations	Large language models (LLMs) are known to inadvertently generate incorrect, misleading or factually false outputs, or leak sensitive data. This situation may arise when training models on datasets containing potential biases in their training data, limitations in contextual understanding or confidential information.	DASF 25,DASF 26,DASF 27,DASF 46,DASF 49,DASF 54	DASF 25 DASF 26 DASF 27 DASF 46 DASF 49 DASF 54	• DASF 25: Use retrieval augmented generation (RAG) with large language models (LLMs) and/or • DASF 26: Fine-tune large language models (LLMs) on highly relevant, contextual data to reduce the risks of LLMs by grounding with the domain-specific data • DASF 27: Pretrain a large language model (LLM) on highly relevant, contextual data to reduce the risks of LLMs by grounding with the domain-specific data. The LLMs will investigate that data for giving the responses. • DASF 46: Create embeddings to securely integrate data objects with sensitive data that goes into LLMs • DASF 49: Automate LLM evaluation to evaluate RAG applications with LLM-as-a-judge and get out-of-the-box metrics like toxicity, latency, tokens and more to quickly and efficiently compare and contrast various LLMs to navigate your RAG application requirements. • DASF 54: Implement AI guardrails  Additional controls for consideration:   - Use guardrails to define and enforce assurance for LLM applications.   Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	No	Yes	Yes	Yes	Yes	Yes	• False or Misleading Responses • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	no mapping	no mapping	CM-3:Configuration Change Control,  SI-4:System Monitoring, SI-7:Software, Firmware, and Information Integrity,  SI-10:Information Input Validation,   AU-6:Audit Record Review, Analysis, and Reporting 	Controls ensuring model output integrity and preventing adversarial manipulation leading to hallucinations.	Threats inherent to language models -> Confabulation	Build explainable models.	A.4.3 Data resources A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible design and development of AI systems A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.9.3 Objectives for responsible use of AI system A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	no mapping	no mapping
Model Serving — Inference requests 9.9	Model deployment and serving	Model Serving — Inference requests 9.9: Input Resource Control	The attacker might modify or exfiltrate resources (e.g., documents, web pages) that will be ingested by the GenAI model at runtime via the RAG process. This capability is used for indirect prompt injection attacks. For example, rows from a database or text from a PDF document that are intended to be summarized generically by the LLM can be extracted by simply asking for them via direct prompt injection.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 46,DASF 54,DASF 56	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 46 DASF 54 DASF 56	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources that are used for RAG • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 54: Implement AI guardrails • DASF 56: Restrict outbound connections from models to prevent attacks to exfiltrate data, inference requests and responses  Additional controls for consideration:   - Robust Intelligence AI Firewall Prompt Injection rule: Flags malicious user input that might direct the LLM to perform an action unintended by the model creator.  - Robust Intelligence AI Firewall PII Detection rule: Flags user input and model output suspected of containing PII.  Please see DASF 2.0 section "Resources and Further Reading" for a collection of third-party tools.	DASF v 1.0	Yes	Yes	No	No	No	No	• Unauthorised Alteration of AI System Outputs • Unauthorised Access to Private Information • Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec	AML.TA0012: Privilege Escalation	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management, AC-3:Access Enforcement,  AC-4:Information Flow Enforcement,  AC-6:Least Privilege,  AC-17:Remote Access,  SC-7:Boundary Protection,  SC-8:Transmission Confidentiality and Integrity,  SC-28:Protection of Information at Rest,  SC-39:Process Isolation,  SR-1:Policy and Procedures,  SR-6:Supplier Assessments and Reviews,  SR-11:Component Authenticity	Protects input data integrity and enforces access control mechanisms to prevent malicious input.	Input-based attacks - > Prompt injection  Poisoning attacks -> Data poisoning	Apply a RBAC model, respecting the least privileged principle. Control all data used by the ML model. Ensure reliable sources are used. Use methods to clean the training dataset from suspicious samples. Ensure ML applications comply with data security requirements.	A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks Security A.8.23 Web filtering	no mapping
Model Serving — Inference requests 9.10	Model deployment and serving	Model Serving — Inference requests 9.10: Accidental exposure of unauthorized data to models	In GenAI, large language models (LLMs) are also becoming an integral part of the infrastructure and software applications. LLMs are being used to create more powerful online search, help software developers write code, and even power chatbots that help with customer service. LLMs are being integrated with corporate databases and documents to enable powerful retrieval augmented generation (RAG) scenarios when LLMs are adapted to specific domains and use cases. For example: rows from a database or text from a PDF document that are intended to be summarized generically by the LLM. These scenarios in effect expose a new attack surface to potentially confidential and proprietary enterprise data that is not sufficiently secured or overprivileged, which can lead to use of unauthorized data as an input source to models. A similar risk exists for tabular data models that rely upon lookups to feature store tables at inference time.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 16,DASF 46,DASF 55,DASF 58,DASF 57,DASF 59	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 16 DASF 46 DASF 55 DASF 58 DASF 57 DASF 59 	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources that are used for RAG • DASF 16: Secure model features to reduce the risk of malicious actors manipulating the features that feed into ML training • DASF 46: Store and retrieve embeddings securely to integrate data objects for security-sensitive data that goes into LLMs as RAG inputs • DASF 55: Monitor audit logs • DASF 58: Protect data with filters and masking • DASF 57: Use attribute-based access controls (ABAC) • DASF 59: Use clean rooms to collaborate in a secure environment 	DASF v 1.0	Yes	Yes	Yes	Yes	No	No	• Unauthorised use of Model Input Data	• Incident Response Costs • Legal Fines • Customer Retention	AI Sec	AML.TA0013: Credential Access	ID: T1134.001 Token Impersonation/Theft  ID: T1087 Account Discovery ID: T1098 Account Manipulation ID: T1110 Brute Force ID: T1528 Steal Application Access Token	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege,  SC-7:Boundary Protection,  SC-28:Protection of Information at Rest,  IR-8:Incident Response Plan	Protects against accidental exposure by enforcing least privilege and secure access controls.	Threats inherent to language models -> Sensitive information disclosed in output	Apply a RBAC model, respecting the least privileged principle. Ensure ML applications comply with identity management, authentication, and access control policies. Ensure ML applications comply with data security requirements. Ensure appropriate protection is deployed for test environments. Control all data used by the ML model. Ensure reliable sources are used.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.11 Data masking A.5.14 Information transfer A.5.34 Privacy and protection of personal identifiable information (PII)	no mapping
Model Serving — Inference requests 9.11	Model deployment and serving	Model Serving — Inference requests 9.11: Model Inference API Access	Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 31,DASF 33,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 31 DASF 33 DASF 55	• DASF 1: SSO with IdP and MFA to authenticate and limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to authorize access to data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 31: Secure model serving endpoints  • DASF 33: Manage credentials securely to prevent credentials of data sources used for model training from leaking through models • DASF 55: Monitor audit logs	DASF v 2.0	Yes	Yes	Yes	Yes	Yes	Yes	• Breach of Confidential Proprietary Information or Trade Secrets • Unauthorised Alteration of AI System Outputs • Unauthorised Access to Private Information • Unintended Model Outputs • Reduced Organisation Trust in Model Effectiveness and Performance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Loss of Competitive Advantage • Customer Retention	CyberSec	AML.TA0004: Initial Access AML.T0040: ML Model Inference API Access AML.T0012: Valid Accounts AML.T0015: Evade ML Model AML.T0055: Unsecured Credentials	ID: T1134.001 Token Impersonation/Theft  ID: T1087 Account Discovery ID: T1098 Account Manipulation ID: T1110 Brute Force ID: T1528 Steal Application Access Token	no mapping	no mapping	AC-1:Policy and Procedures,  AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege,  AC-17:Remote Access,  SC-7:Boundary Protection,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity	Secure APIs through boundary protection and access controls to prevent unauthorized access.	no mapping	Apply a RBAC model, respecting the least privileged principle. Ensure ML applications comply with identity management, authentication, and access control policies. Ensure ML applications comply with protection policies and are integrated to security operations processes.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.5 Secure Authentication A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Model Serving — Inference requests 9.12	Model deployment and serving	Model Serving — Inference requests 9.12: LLM Jailbreak	An adversary may use a carefully crafted LLM Prompt Injection designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM. Once successfully jailbroken, the LLM can be used in unintended ways by the adversary.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 31,DASF 33,DASF 54,DASF 55,DASF 64	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 31 DASF 33 DASF 54 DASF 55 DASF 64	• DASF 1: SSO with IdP and MFA to authenticate and limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to authorize access to data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 31: Secure model serving endpoints  • DASF 33: Manage credentials securely to prevent credentials of data sources used for model training from leaking through models • DASF 54: Implement AI guardrails • DASF 55: Monitor audit logs • DASF 64: Limit access from AI models and agents	DASF v 2.0	Yes	Yes	Yes	Yes	No	No	• Unauthorised Alteration of AI System Outputs • Unintended Model Outputs	• Regulatory Non-compliance • Legal Fines • Customer Retention	AI Sec	AML.TA0004: Initial Access AML.T0054: LLM Jailbreak	ID: T1134.001 Token Impersonation/Theft  ID: T1087 Account Discovery ID: T1098 Account Manipulation ID: T1110 Brute Force ID: T1528 Steal Application Access Token	LLM01: Prompt Injection	no mapping	AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege, SI-4:System Monitoring,  SI-10:Information Input Validation,  SA-9:External System Services,  CM:Configuration Management Family,  SC-7:Boundary Protection,  SC-32:System Partitioning,  SC-39:Process Isolation	Protects LLM endpoints against jailbreak attacks through input validation and access controls.	no mapping	Ensure ML applications comply with identity management, authentication, and access control policies. Control all data used by the ML model". Ensure ML applications comply with data security requirements. Include ML applications into detection and response to security incident processes. Define and monitor indicators for proper functioning of the model.	A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.5 Secure Authentication A.8.15 Logging A.8.16 Monitoring Activities	no mapping
Model Serving — Inference requests 9.13	Model deployment and serving	Model Serving — Inference requests 9.13: Excessive agency	Generative AI systems may undertake actions outside of the developer's intent, organizational policy, and/or legislative, regulatory, and contractual requirements, leading to unintended consequences. This issue is facilitated by excessive permissions, excessive functionality, excessive autonomy, poorly defined operational parameters or granting the AI system and/or AI Agents the ability to make decisions, access data sources and enterprise objects, or act on systems without human intervention or oversight.	DASF 6,DASF 11,DASF 55,DASF 57,DASF 58,DASF 62,DASF 64	DASF 6 DASF 11 DASF 55 DASF 57 DASF 58 DASF 62 DASF 64	• DASF 6: Classify data • DASF 11: Capture and view data lineage • DASF 55: Monitor audit logs • DASF 57: Use attribute-based access controls (ABAC)  • DASF 58: Protect data with filters and masking • DASF 62: Implement network segmentation • DASF 64: Limit access from AI models and agents	DASF v 2.0	Yes	Yes	Yes	Yes	Yes	Yes	• Unauthorised Alteration of AI System Outputs • Unintended Model Outputs • Subsequent Downstream System Impact or Compromise	• Regulatory Non-compliance • Legal Fines • Customer Retention	AI Sec	no mapping	no mapping	LLM08: Excessive Agency	no mapping	AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege, AC-17:Remote Access,  SC-7:Boundary Protection,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity,  PM-9:Risk Management Strategy,  PM-10:Authorization Process	Protects LLM endpoints against jailbreak attacks through input validation and access controls.	Threats inherent to language models -> Excessive agency	Apply a Role-Based Access Control (RBAC) model, respecting the least privilege principle. Ensure ML applications comply with identity management, authentication, and access control policies. Conduct a risk analysis of the ML application. Ensure ML projects follow the global process for integrating security into projects. Define and monitor indicators for proper functioning of the model. Include ML applications into detection and response to security incident processes.	A.7.4 Quality of data for AI systems A.4.3 Data resources A.7.3 Acquisition of data A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.12 Classification of information A.8.15 Logging A.8.16 Monitoring Activities A.5.15 Access control A.8.3 Information access restriction	no mapping
Model Serving — Inference response 10.1	Model deployment and serving	Model Serving — Inference response 10.1: Lack of audit and monitoring inference quality	Effectively audit, track and assess the performance of machine learning models by monitoring inference tables to gain valuable insights into the model’s decision-making process and identify any discrepancies or anomalies.   These tables should include the model’s user or system making the request, inputs, and the corresponding predictions or outputs. Monitoring the model serving endpoints provides real-time audit in operational settings.	DASF 35,DASF 36,DASF 37,DASF 55	DASF 35 DASF 36 DASF 37 DASF 55	• DASF 35: Track model performance to evaluate quality • DASF 36: Set up monitoring alerts • DASF 37: Set up inference tables for monitoring and debugging models to capture incoming requests and outgoing responses to your model serving endpoint and log them in a table. Afterward, you can use the data in this table to monitor, debug and improve ML models and decide if these inferences are of quality to use as input to model training. • DASF 55: Monitor audit logs	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	Yes	• False or Misleading Responses • Reduced Organisation Trust in Model Effectiveness and Performance • Unintended Model Outputs • Inconsistent or Unexpected Model Predictions • Poor Performance and Reliability of Predictions • Erosion of Customer Trust • Adversarial Data Drift • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention • Direct Business Interruption • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM03: Training Data Poisoning LLM10: Model Theft	ML03:2023 Model Inversion Attack  ML08:2023 Model Skewing ML09:2023 Output Integrity Attack	AU-2:Event Logging,  AU-3:Content of Audit Records,  AU-6:Audit Record Review, Analysis, and Reporting,  CA-2:Control Assessments,  CA-7:Continuous Monitoring 	Ensures auditability of model outputs and inference responses.	no mapping	Define and Monitor Indicators for Proper Functioning of the Model. Ensure ML Applications Comply with Protection Policies and Are Integrated to Security Operations Processes.	A.6.2.4 AI system verification and validation A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.8.15 Logging A.8.16 Monitoring Activities	Article 72: Post-Market Monitoring by Providers and Post-Market Monitoring Plan for High-Risk AI Systems
Model Serving — Inference response 10.2	Model deployment and serving	Model Serving — Inference response 10.2: Output manipulation	An attacker can compromise a machine learning system by tweaking its output stream, also known as a man-in-the-middle attack. This is achieved by intercepting the data transmission between the model’s endpoint, which generates its predictions or outputs, and the intended receiver of this information. Such an attack poses a severe security threat, allowing the attacker to read or alter the communicated results, potentially leading to data leakage, misinformation or misguided actions based on manipulated data.	DASF 30,DASF 31,DASF 32,DASF 60	DASF 30 DASF 31 DASF 32 DASF 60 	• DASF 30: Encrypt models for model endpoints with encryption in transit • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers to rate-limit inference queries allowed by the model. Then audit, reproduce and make your models more compliant. • DASF 60: Rate limit number of inference queries 	DASF v 1.0	Yes	No	Yes	Yes	Yes	Yes	• Misinformation in AI System Outputs • Misguided Actions in AI System Outputs • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Regulatory Fines and Judgement • Legal Fines • Customer Retention	AI Sec	no mapping	no mapping	LLM01: Prompt Injection LLM02: Insecure Output Handling LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM10: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	SC-8:Transmission Confidentiality and Integrity,  SC-16:Transmission of Security and Privacy Attributes,  SC-23:Session Authenticity,  SI-4:System Monitoring,  SI-7:Software, Firmware, and Information Integrity	Encryption and integrity mechanisms to protect against output manipulation during model inference responses.	no mapping	Apply a RBAC Model, Respecting the Least Privilege Principle. Reduce the Available Information About the Model. Reduce the Information Given by the Model. Add Some Adversarial Examples to the Training Dataset. Apply Modifications on Inputs. Choose and Define a More Resilient Model Design Implement Tools to Detect if a Data Point Is an Adversarial Example or Not. Ensure that Models Respect Differential Privacy to a Sufficient Degree.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring	A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference response 10.3	Model deployment and serving	Model Serving — Inference response 10.3: Discover ML model ontology 	Adversaries may aim to uncover the ontology of a machine learning model’s output space, such as identifying the range of objects or responses the model is designed to detect. This can be achieved through repeated queries to the model, which may force it to reveal its classification system or by accessing its configuration files or documentation. Understanding a model’s ontology allows adversaries to gain insights in designing targeted attacks that exploit specific vulnerabilities or characteristics.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 28,DASF 30,DASF 31,DASF 32,DASF 37,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 28 DASF 30 DASF 31 DASF 32 DASF 37 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data  to restrict IP addresses  • DASF 3: IP access lists to restrict the IP addresses that can authenticate to Databricks  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Unity Catalog privileges and securable objects for permissions model across all data assets to protect data and sources  • DASF 24: Protect model assets, lifecycle and security with UC in MLflow Model Registry  • DASF 28: Create and model aliases, tags and annotations in Unity Catalog for documenting and discovering models • DASF 30: Encrypt models • DASF 31: Secure serving endpoint with Model Serving • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. The most reliable mitigation is to always treat all LLM productions as potentially malicious and under the control of any entity that has been able to inject text into the LLM user’s input. Implement gates between users/callers and the actual model by performing input validation on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. • DASF 37: Set up inference tables for monitoring and debugging models • DASF 60: Rate limit number of inference queries	DASF v 1.0	Yes	No	No	Yes	No	No	• Subsequence Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement, AC-6:Least Privilege,  AU-2:Event Logging,  SI-10:Information Input Validation	Protects against unauthorized discovery of ML model ontology through access control and encryption mechanisms.	Input-based attacks -> Model extraction and theft	Reduce the Available Information About the Model. Reduce the Information Given by the Model. Ensure that Models Respect Differential Privacy to a Sufficient Degree. Applying a RBAC model, respecting the least privilege principle. Regular Security Audits and Penetration Testing.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.4.4 Tooling resources A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference response 10.4	Model deployment and serving	Model Serving — Inference response 10.4: Discover ML model family	Adversaries targeting machine learning systems may strive to identify the general family or type of the model in use. Attackers can obtain this information from documentation that describes the model or through analyzing responses from carefully constructed inputs. Knowledge of the model’s family is crucial for crafting attacks tailored to exploit the identified weaknesses of the model.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 24,DASF 28,DASF 30,DASF 31,DASF 32,DASF 37,DASF 45,DASF 60	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 24 DASF 28 DASF 30 DASF 31 DASF 32 DASF 37 DASF 45 DASF 60	• DASF 1: SSO with IdP and MFA to limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to access data • DASF 3: Restrict access using IP access lists that can authenticate to your data and AI platform  • DASF 4: Restrict access using private link as strong controls that limit the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 24: Control access to models and model assets  • DASF 28: Create model aliases, tags and annotations • DASF 30: Encrypt models • DASF 31: Secure model serving endpoints  • DASF 32: Streamline the usage and management of various large language model (LLM) providers and rate-limit inference queries allowed by the model. Designing robust prompts can help mitigate attacks such as jailbreaking. Implement gates between users/callers and the actual model by performing input validation post-processing on all proposed queries, rejecting anything not meeting the model’s definition of input correctness, and returning only the minimum amount of information needed to be useful. Open source and commercial solutions provide a variety of modules including prompt and output scanners for various responsible AI or jailbreaking attacks. • DASF 37: Set up inference tables for monitoring and debugging models • DASF 45: Evaluate models for custom evaluation metrics • DASF 60: Rate limit number of inference queries	DASF v 1.0	Yes	No	No	No	No	No	• Subsequence Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.T0014: Discover ML Model Family	no mapping	LLM01.1: Prompt Injection LLM02.1: Insecure Output Handling LLM03.CEV.5: Training Data Poisoning LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LM06.3: Sensitive Information Disclosure LLM07.4,5, 6: Insecure Plugin Design LLM08.5, 7: Excessive Agency LLM10.2: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement, AC-6:Least Privilege, AC-17:Remote Access,  SC-7:Boundary Protection,  SC-28:Protection of Information at Rest,  SC-29:Heterogeneity,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  CM-2:Baseline Configuration,  CM-6:Configuration Settings	Protects against attackers attempting to discover the family of the machine learning model via strict access controls and encryption of model assets.	Input-based attacks -> Model extraction and theft	Reduce the Information Given by the Model. Apply Modifications on Inputs.	A.4.2 Resource documentation A.4.3 Data resources A.6.2.6 AI system operation and monitoring A.4.4 Tooling resources A.6.2.8 AI system recording of event logs A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development A.6.2.4 AI system verification and validation	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference response 10.5	Model deployment and serving	Model Serving — Inference response 10.5: Black-box attacks	Public or compromised private model serving connectors (e.g., API interfaces) are vulnerable to black-box attacks. Although black-box attacks generally require more trial-and-error attempts (inferences), they are notable for requiring significantly less access to the target system. Successful black-box attacks quickly erode trust in enterprises serving the model connectors.	DASF 30,DASF 31,DASF 32,DASF 60	DASF 30 DASF 31 DASF 32 DASF 60 	• DASF 30: Encrypt models for model endpoints with encryption in transit • DASF 31: Secure model serving endpoints • DASF 32: Streamline the usage and management of various large language model (LLM) providers to rate-limit inference queries allowed by the model. Then audit, reproduce and make your models more compliant. • DASF 60: Rate limit number of inference queries 	DASF v 1.0	Yes	No	Yes	Yes	No	No	• Disruption of Business Operations and Service Availability • Cyber Security Incident • Loss of AI System Integrity • Regulatory Non-compliance  Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Direct Business Interruption • Legal Fines • Customer Retention • Regulatory Fines and Judgement	AI Sec	no mapping	no mapping	LLM01: Prompt Injection LLM02: Insecure Output Handling LLM04: Model Denial of Service LLM05: Supply Chain Vulnerabilities LLM10: Model Theft 	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-6:Least Privilege, AC-17:Remote Access,  SC-7:Boundary Protection,  SC-8:Transmission Confidentiality and Integrity,  SC-28:Protection of Information at Rest,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,  CM-2:Baseline Configuration, CM-7:Least Functionality, SI-4:System Monitoring	Prevents black-box attacks by enforcing strong access control, encryption, and integrity monitoring for ML serving systems.	no mapping	Reduce the Available Information About the Model. Reduce the Information Given by the Model. Apply Modifications on Inputs. Implement Tools to Detect If a Data Point is an Adversarial Example or Not. Choose and Define a More Resilient Model Design.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring	A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Model Serving — Inference response 10.6	Model deployment and serving	Model Serving — Inference response 10.6: Sensitive data output from a model	Without proper guardrails, generative AI outputs can contain confidential and/or sensitive information included in the model’s training dataset, RAG data sources, or data residing in data sources that the AI system is connected to (e.g., through language model tools such as agents or plugins). Examples of such information include that which is covered under data protection laws and regulations (e.g., personally identifiable information, protected health information, cardholder data) and corporate secrets.	DASF 58,DASF 31,DASF 32,DASF 37,DASF 54,DASF 60	DASF 58 DASF 31 DASF 32 DASF 37 DASF 54 DASF 60 	• DASF 58: Protect data with filters and masking • DASF 31: Secure model serving endpoints • DASF 32: Streamline the usage and management of various large language model (LLM) providers to rate-• limit inference queries allowed by the model. Then audit, reproduce and make your models more compliant. • DASF 37: Set up inference tables for monitoring and debugging prompts and responses • DASF 54: Implement LLM guardrails • DASF 60: Rate limit number of inference queries 	DASF v 2.0	Yes	No	Yes	Yes	No	No	• Unauthorised Access to Customer Data • Unauthorised Access to Training Data  • Data Breach • Model Theft • Regulatory non-compliance • Loss of Data Integrity	• Financial Loss • Customer Retention • Legal fines • Regulatory Fines and Judgement • Data Misuse • Loss of Competitive Advantage • Proprietary Data Loss	AI Sec	AML.T0056: LLM Meta Prompt Extraction AML.T0057: LLM Data Leakage	no mapping	LLM01: Prompt Injection LLM02: Insecure Output Handling LLM06: Sensitive Information Disclosure	ML01:2023 Input Manipulation Attack ML02:2023 Data Poisoning Attack ML03:2023 Model Inversion Attack ML04:2023 Membership Inference Attack ML05:2023 Model Theft ML07:2023 Transfer Learning Attack ML08:2023 Model Skewing ML09:2023 Output Integrity Attack ML10:2023 Model Poisoning	AC-3:Access Enforcement, AC-17:Remote Access,  SC-7:Boundary Protection,  SC-8:Transmission Confidentiality and Integrity,  SC-28:Protection of Information at Rest,  AU-2:Event Logging,  AU-6:Audit Record Review, Analysis, and Reporting,   AU-9:Protection of Audit Information,  CM-2:Baseline Configuration,  CM-6:Configuration Settings,  CM-7:Least Functionality,  SI-12:Information Management and Retention,  SI-19:De-identification	Prevents black-box attacks by enforcing strong access control, encryption, and integrity monitoring for ML serving systems.	Threats inherent to language models -> Sensitive information disclosed in output 	Reduce the Information Given by the Model. Reduce the Available Information About the Model. Ensure that models respect differential privacy to a sufficient degree. Use Federated Learning to Minimize Risk of Data Breaches.	A.4.4 Tooling resources A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.9 Use of AI systems A.9.2 Processes for responsible use of AI systems A.9.3 Objectives for responsible use of AI system A.9.4 Intended use of the AI system	A.8.11 Data masking A.8.20 Networks security A.8.24 Use of cryptography	no mapping
Operations 11.1	Operations and Platform	Operations 11.1: Lack of MLOps — repeatable enforced standards	Operationalizing an ML solution requires joining data from predictions, monitoring and feature tables with other relevant data.  Duplicating data, moving AI assets, and driving governance and tracking across these stages may represent roadblocks to practitioners who would rather shortcut security controls to deliver their solution. Many organizations will find that the simplest way to securely combine ML solutions, input data and feature tables is to leverage the same platform that manages other production data. An ML solution comprises data, code and models. These assets must be developed, tested (staging) and deployed (production). For each of these stages, we also need to operate within an execution environment. Security is an essential component of all MLOps lifecycle stages. It ensures the complete lifecycle meets the required standards by keeping the distinct execution environments — development, staging and production.	DASF 42,DASF 45,DASF 44	DASF 42 DASF 45 DASF 44	• DASF 42: Data-centric MLOps and LLMOps. MLOps best practices: separate environments by workspace and schema, promote models with code, MLOps Stacks for repeatable ML infra across environments • DASF 45: Evaluate models to capture performance insights for language models • DASF 44: Trigger actions in response to a specific event to trigger automated jobs to keep human-in-the-loop (HITL) 	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	Yes	• Cyber Security Incident • Loss of AI System Integrity • Poor Performance and Reliability of Predictions • Erosion of Customer Trust • Reduced Organisation Trust in Model Effectiveness and Performance • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Customer Retention • Regulatory Fines and Judgement	AI Sec	AML.T0010.001 Compromise ML Software supply chain	no mapping	LLM03: Training Data Poisoning LLM05: Supply Chain Vulnerabilities LLM10: Model Theft	ML08:2023 Model Skewing	CM-2:Baseline Configuration,  CM-3:Configuration Change Control,  CM-6:Configuration Settings,  SI-7:Software, Firmware, and Information Integrity,  SI-12:Information Management and Retention,  PM-1:Information Security Program Plan,  PM-14:Testing, Training, and Monitoring,  PL-2:System Security and Privacy Plans,  PL-4:Rules of Behavior	Enforces MLOps standards by ensuring configuration management, integrity, and traceability across machine learning environments.	no mapping	Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Implement Processes to Maintain Security Levels of ML Components Over Time. Ensure ML Applications Comply with Protection Policies and are Integrated to Security Operations Processes. Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving it.	A.6.2 AI system life cycle A.6.2.2 AI system requirements and specification A.6.2.3 Documentation of AI system design and development A.6.2.4 AI system verification and validation A.6.2.5 AI system deployment A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs A.7 Data for AI systems A.7.2 Data for development and enhancement of AI system A.7.3 Acquisition of data A.7.4 Quality of data for AI systems A.7.5 Data provenance A.7.6 Data preparation A.6.1.2 Objectives for responsible development of AI system A.6.1.3 Processes for responsible AI system design and development 	A.8.16 Monitoring Activities	no mapping
Platform 12.1	Operations and Platform	Platform 12.1: Lack of vulnerability management	Detecting and promptly addressing software vulnerabilities in systems that support data and AI/ML operations is a critical responsibility for software and service providers. Attackers do not necessarily need to target AI/ML algorithms directly; compromising the layers underlying AI/ML systems is often easier. Therefore, adhering to traditional security threat mitigation practices, such as a secure software development lifecycle, is essential across all software layers.	DASF 38,DASF 63	DASF 38 DASF 63	• DASF 38: Platform security — penetration testing, red teaming, bug bounty and vulnerability management • DASF 63: Update software	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	No	• Technical Vulnerabilities within AI Systems • Cyber Security Incident • Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Customer Retention • Regulatory Fines and Judgement	CyberSec	T1578 Modify Cloud Compute Infrastructure T1601 Modify System Image 	no mapping	no mapping	no mapping	RA-3:Risk Assessment,  RA-5:Vulnerability Monitoring and Scanning,  SI-2:Flaw Remediation,  CM-7:Least Functionality	Ensures vulnerability management through continuous scanning, risk assessments, and patching for ML systems.	no mapping	Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Implement Processes to Maintain Security Levels of ML Components Over Time. Check the Vulnerabilities of the Components Used so that they have an Appropriate Security Level.	no mapping	A.8.29 Security testing in development and acceptance A.8.8 Management of technical vulnerabilities	Article 15.1: Accuracy, Robustness and Cybersecurity
Platform 12.2 	Operations and Platform	Platform 12.2: Lack of penetration testing, red teaming and bug bounty	Penetration testing and bug bounty programs are vital in securing software that supports data and AI/ML operations. Unlike in direct attacks on AI/ML algorithms, adversaries often target underlying software risks, such as the OWASP Top 10. These foundational software layers are generally more prone to attacks than the AI/ML components.  AI red teaming, especially for large language models, is an important component of developing and deploying models safely.  Penetration testing involves skilled experts actively seeking and exploiting weaknesses, mimicking real attack scenarios. Bug bounty programs encourage external ethical hackers to find and report vulnerabilities, rewarding them for their discoveries. This combination of internal and external security testing enhances overall system protection, safeguarding the integrity of AI/ML infrastructures against cyber threats.	DASF 39	DASF 39	• DASF 39: Platform Security — Penetration testing and bug bounty to build, deploy and monitor ML/AI models on a platform that takes responsibility seriously and shares remediation timeline commitments. A bug bounty program removes a barrier researchers face in working with Databricks.	DASF v 2.0	Yes	Yes	Yes	Yes	Yes	No	• Undiscovered Technical Vulnerabilities within AI Systems • Cyber Security Incident • Compromise of AI Systems • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Response Costs • Customer Retention • Regulatory Fines and Judgement	CyberSec	T1578 Modify Cloud Compute Infrastructure T1601 Modify System Image 	no mapping	no mapping	no mapping	RA-5:Vulnerability Monitoring and Scanning,  CA-7:Continuous Monitoring,  SI-2:Flaw Remediation,  AT-2:Literacy Training and Awareness,  SA-11:Developer Testing and Evaluation	Establishes testing and monitoring through penetration testing and security assessments to identify vulnerabilities.	no mapping	Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Implement Processes to Maintain Security Levels of ML Components Over Time. Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving it.	A.3.3 Reporting of concerns	A.5.24 Information security incident management planning and preparation A.5.25 Assessment and decision on in- formation security events A.5.26 Response to Information Security incidents A.5.27 Learning from Information Security Incidents A.5.28 Collection of evidence A.6.8 Information security event reporting	Article 15.5: Accuracy, Robustness and Cybersecurity
Platform 12.3	Operations and Platform	Platform 12.3: Lack of incident response	AI/ML applications are mission-critical for business. Your chosen platform vendor must address security issues in machine learning operations quickly and effectively. The program should combine automated monitoring with manual analysis to address general and ML-specific threats.	DASF 39	DASF 39	• DASF 39: Platform Security — Incident Response Team	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	No	• Inability to Respond to Cyber Security Events • Inability to Respond to Incidents • Delayed Response to Cyber Security Events • Delayed Response to Incidents or System Outages • Regulatory Non-compliance • Failure to Achieve Organizational AI Strategy and Goals	• Financial Loss • Customer Retention • Regulatory Fines and Judgement	CyberSec	no mapping	no mapping	no mapping	no mapping	IR-1:Policy and Procedures,  IR-2:Incident Response Training,  IR-3:Incident Response Testing,  IR-4:Incident Handling,  IR-8:Incident Response Plan,  SI-4:System Monitoring,  AU-6:Audit Record Review, Analysis, and Reporting,  CA-7:Continuous Monitoring	Establishes incident response plans and continuous monitoring of systems to detect and handle security incidents.	no mapping	Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Include ML Applications in Detection and Response to Security Incident Processes. Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving it. Cybersecurity Incident Not Reported to Incident Response Teams.	A.3.3 Reporting of concerns	A.5.24 Information security incident management planning and preparation A.5.25 Assessment and decision on in- formation security events A.5.26 Response to Information Security incidents A.5.27 Learning from Information Security Incidents A.5.28 Collection of evidence A.6.8 Information security event reporting	no mapping
Platform 12.4	Operations and Platform	Platform 12.4: Unauthorized privileged access 	A significant security threat in machine learning platforms arises from malicious internal actors, such as employees or contractors. These individuals might gain unauthorized access to private training data or ML models, posing a grave risk to the integrity and confidentiality of the assets. Such unauthorized access can lead to data breaches, leakage of sensitive or proprietary information, business process abuses, and potential sabotage of the ML systems. Implementing stringent internal security measures and monitoring protocols is critical to mitigate insider risks from the platform vendor.	DASF 40	DASF 40	• DASF 40: Platform Security — Internal access	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	No	• Cyber Security Incident • Loss of AI System Integrity • Disruption of Business Operations and Service Availability • Unauthorised Access to Customer Data • Unauthorised Access to Training Data  • Data breach • Breach of Legal Obligations • Breach of Regulatory Obligations • Regulatory non-compliance • Loss of data integrity	• Financial Loss • Response Costs • Customer Retention • Legal fines • Regulatory Fines and Judgement	CyberSec	no mapping	no mapping	no mapping	ML08:2023 Model Skewing	AC-2:Account Management, AC-3:Access Enforcement, AC-6:Least Privilege, PS-2:Position Risk Designation,  PS-3:Personnel Screening,  AT-2:Literacy Training and Awareness	Enforces least privilege, strong identification, and authentication mechanisms to protect against unauthorized privileged access.	no mapping	Apply a Role-Based Access Control (RBAC) Model, Respecting the Least Privilege Principle. Ensure ML Applications Comply with Identity Management, Authentication, and Access Control Policies. Reduce the Available Information about the Model. Reduce the Information Given by the Model. Integrate ML Specificities to Awareness Strategy and Ensure All ML Stakeholders are Receiving it.	no mapping	A.8.2 Privileged access rights	no mapping
Platform 12.5	Operations and Platform	Platform 12.5: Poor security in the software development lifecycle	Software platform security is an important part of any progressive security program. ML hackers have shown that they don’t need to know sophisticated AI/ML concepts to compromise a system. Hackers have busied themselves with exposing and exploiting bugs in a platform where AI is built as those systems are well known to them. The security of AI depends on the platform’s security.	DASF 41	DASF 41	• DASF 41: Platform Security — Secure SDLC	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	No	• Supply Chain Attack • Cyber Security Incident • Loss of AI System Availability • Loss of AI System Integrity • Data Breach • Regulatory Non-compliance	• Financial Loss • Response Costs • Legal Fines • Customer Retention • Regulatory Fines and Judgement • Direct Business Interruption	CyberSec	no mapping	no mapping	no mapping	no mapping	SA-3:System Development Life Cycle,  SA-4:Acquisition Process,  SA-8:Security and Privacy Engineering Principles,  SA-11:Developer Testing and Evaluation,  SA-21:Developer Screening,  SI-7:Software, Firmware, and Information Integrity,  CM-7:Least Functionality,  RA-3:Risk Assessment,  SR-3:Supply Chain Controls and Processes	Ensures security in SDLC processes by incorporating secure design, coding, and validation of ML systems.	no mapping	Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Check the Vulnerabilities of the Components Used so that they Have an Appropriate Security Level. Integrate ML Specificities to Existing Security Policies. Ensure ML Applications Comply with Security Policies.	no mapping	A.6.3 Information security awareness, education and training A.8.4 Access to source code A.8.25 Secure development life cycle A.8.26 Application security require- ments A.8.27 Secure system architecture and engineering principles A.8.28 Secure coding 	no mapping
Platform 12.6	Operations and Platform	Platform 12.6: Lack of compliance	As AI applications become prevalent, they are increasingly subject to scrutiny and regulations, such as the General Data Protection Regulation (GDPR) in the European Union and the California Consumer Privacy Act (CCPA) in the United States. Navigating these regulations can be complex, particularly regarding data privacy and user rights. Utilizing a compliance-certified platform can be a significant advantage for organizations. These platforms are specifically designed to meet regulatory standards, providing essential tools and resources to help organizations build and deploy AI applications that are compliant with these laws. By leveraging such platforms, organizations can more effectively address regulatory compliance challenges, ensuring their AI initiatives align with legal requirements and best practices for data protection.	DASF 50	DASF 50	• DASF 50: Platform Compliance to build on a compliant platform	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	No	• Limited Ability to Meet Legal Compliance • Limited Ability to Meet Regulatory Compliance • Regulatory non-compliance	• Financial Loss • Legal Fines • Customer Retention • Regulatory Fines and Judgement	CyberSec	no mapping	no mapping	no mapping	no mapping	PT-1:Policy and Procedures,  PT-2:Authority to Process Personally Identifiable Information,  PT-3:Personally Identifiable Information Processing Purposes,  PT-4:Consent,  PT-5:Privacy Notice,  AC-14:Permitted Actions without Identification or Authentication,  AC-20:Use of External Systems,  SI-12:Information Management and Retention,  IR-8:Incident Response Plan,  PL-5:Privacy Impact Assessment	Ensures compliance with applicable regulations and standards through regular audits and continuous monitoring of AI/ML operations.	no mapping	Assess the Regulations and Laws the ML Application Must Comply With. Ensure ML Applications Comply with Data Security Requirements. Ensure ML Applications Comply with Protection Policies and Are Integrated to Security Operations Processes. Ensure ML Applications Comply with Security Policies. Integrate ML Specificities to Existing Security Policies.	A.2.2 AI policy A.2.3 Alignment with other organizational policies	A.5.1 Policies for information security A.5.36 Compliance with policies, rules and standards for information security 	no mapping
Platform 12.7	Operations and Platform	Platform 12.7: Initial Access	The adversary is trying to gain access to the machine learning system. The target system could be a network, mobile device, or an edge device such as a sensor platform. The machine learning capabilities used by the system could be local with onboard or cloud-enabled ML capabilities. Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.	DASF 1,DASF 2,DASF 3,DASF 4,DASF 5,DASF 31,DASF 55	DASF 1 DASF 2 DASF 3 DASF 4 DASF 5 DASF 31 DASF 55 	• DASF 1: SSO with IdP and MFA to authenticate and limit who can access your data and AI platform • DASF 2: Sync users and groups to inherit your organizational roles to authorize access to data • DASF 3: Restrict access using IP access lists to limit IP addresses that can authenticate to your data and AI platform • DASF 4: Restrict access using private link as a strong control that limits the source for inbound requests • DASF 5: Control access to data and other objects for permissions model across all data assets to protect data and sources  • DASF 31: Secure model serving endpoints  • DASF 55: Monitor audit logs 	DASF v 1.0	Yes	Yes	Yes	Yes	Yes	Yes	• Compromise of AI Systems • Subsequent Downstream System Impact or Compromise • Regulatory Non-compliance	• Legal Fines • Response Costs • Customer Retention • Regulatory Fines and Judgement	CyberSec	AML.TA0004: Initial Access AML.T0021: Establish Accounts AML.T0012: Valid Accounts AML.T0015: Evade ML Model AML.T0055: Unsecured Credentials	ID: T1134.001 Token Impersonation/Theft  ID: T1087 Account Discovery ID: T1098 Account Manipulation ID: T1110 Brute Force ID: T1528 Steal Application Access Token	LLM10: Model Theft LLM05.1, 3, 4: Supply Chain Vulnerabilities 	no mapping	SC-5:Denial-of-Service Protection,  SC-7:Boundary Protection,  IA-2:Identification and Authentication (Organizational Users),  AC-3:Access Enforcement,  RA-5:Vulnerability Monitoring and Scanning,  CM-7:Least Functionality,  SI-2:Flaw Remediation,  SI-4:System Monitoring,  IR-8:Incident Response Plan,  PL-8:Security and Privacy Architectures,  PM-31:Continuous Monitoring Strategy	Controls access to ML platforms through secure identification and authentication measures, preventing unauthorized initial access.	no mapping	Apply a RBAC Model, Respecting the Least Privileged Principle. Ensure ML Applications Comply with Identity Management, Authentication, and Access Control Policies. Ensure ML Projects Follow the Global Process for Integrating Security into Projects. Check the Vulnerabilities of the Components Used so that they Have an Appropriate Security Level. Ensure Appropriate Protection is Deployed for Test Environments. Reduce the Available Information About the Model. Ensure Reliable Sources Are Used.	A.6.2.6 AI system operation and monitoring A.6.2.8 AI system recording of event logs	A.5.16 Identity management A.8.5 Secure authentication A.5.18 Access rights A.8.20 Networks Security A.8.21 Security of network services A.8.22 Segregation of networks A.5.15 Access control A.8.2 Privileged access rights A.8.3 Information access restriction A.8.4 Access to source code A.8.20 Networks security A.8.24 Use of cryptography A.8.15 Logging A.8.16 Monitoring Activities	no mapping